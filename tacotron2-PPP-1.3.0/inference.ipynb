{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Start) Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nWhat the fuck did you just fucking say about me, you little pony?', 'I’ll have you know I graduated top of my class in magic kindergarten, and I’ve been involved in numerous secret raids on Nightmare Moon, and I have over 300 confirmed friendships.', 'I am trained in magic warfare and I’m the top pony in the entire Equestrian armed forces.', 'You are nothing to me but just another friend.', 'I will wipe you the fuck out with friendship the likes of which has never been seen before on Equestria, mark my fucking words.', 'You think you can get away with saying that shit to me over the Ponynet?', 'Think again, fucker.', 'As we speak I am contacting my secret network of pegasi across Equestria and your hoofprints are being traced right now so you better prepare for the storm, maggot.', 'The storm that wipes out the pathetic little thing you call your life.', 'You’re fucking dead, pony.', 'I can be anywhere, anytime, and I can hug you in over seven hundred ways, and that’s just with my bare hooves.', 'Not only am I extensively trained in unarmed friendship, but I have access to the entire arsenal of ponies and I will use it to its full extent to wipe your miserable flank off the face of the continent, you little pony.', 'If only you could have known what magical friendship your little “clever” comment was about to bring down upon you, maybe you would have held your fucking tongue.', 'But you couldn’t, you didn’t, and now you’re paying the price, you goddamn pony.', 'I will shit friendship all over you and you will drown in it.', 'You’re fucking dead, pony.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text = \"\"\"\n",
    "What the fuck did you just fucking say about me, you little pony?\n",
    "I’ll have you know I graduated top of my class in magic kindergarten, and I’ve been involved in numerous secret raids on Nightmare Moon, and I have over 300 confirmed friendships.\n",
    "I am trained in magic warfare and I’m the top pony in the entire Equestrian armed forces.\n",
    "You are nothing to me but just another friend.\n",
    "I will wipe you the fuck out with friendship the likes of which has never been seen before on Equestria, mark my fucking words.\n",
    "You think you can get away with saying that shit to me over the Ponynet?\n",
    "Think again, fucker.\n",
    "As we speak I am contacting my secret network of pegasi across Equestria and your hoofprints are being traced right now so you better prepare for the storm, maggot.\n",
    "The storm that wipes out the pathetic little thing you call your life. You’re fucking dead, pony. I can be anywhere, anytime, and I can hug you in over seven hundred ways, and that’s just with my bare hooves. Not only am I extensively trained in unarmed friendship, but I have access to the entire arsenal of ponies and I will use it to its full extent to wipe your miserable flank off the face of the continent, you little pony. If only you could have known what magical friendship your little “clever” comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn’t, you didn’t, and now you’re paying the price, you goddamn pony. I will shit friendship all over you and you will drown in it. You’re fucking dead, pony.\n",
    "\"\"\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adsoj'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"adsoj   \".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "x = [random.random() < 1.0 for i in range(100000)]\n",
    "x = [y for y in x if y]\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_mask_from_lengths\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 5., 6., 2.], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True, False, False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,5,6,2]).cuda().float()\n",
    "print(x)\n",
    "mask = get_mask_from_lengths(x, max_len=int(x.max().item()))\n",
    "spec = torch.rand(4,160,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (End) Testing Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tacotron 2 inference code \n",
    "Edit the variables **checkpoint_path** and **text** to match yours and run the entire code to generate plots of mel outputs, alignments and audio synthesis from the generated mel-spectrogram using Griffin-Lim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and setup matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "D:\\Miniconda\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Tacotron2\n",
    "from layers import TacotronSTFT, STFT\n",
    "from audio_processing import griffin_lim\n",
    "from train import load_model\n",
    "from text import text_to_sequence\n",
    "from denoiser import Denoiser\n",
    "from unidecode import unidecode\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running, Please wait...\n",
      "Dictionary Ready.\n"
     ]
    }
   ],
   "source": [
    "def plot_data(data, title=None):\n",
    "    %matplotlib inline\n",
    "    if len(data) > 1:\n",
    "        fig, axes = plt.subplots(len(data), 1, figsize=(int(alignment_graph_width*graph_scale/100), int(alignment_graph_height*graph_scale/100)))\n",
    "        axes = axes.flatten()\n",
    "        for i in range(len(data)):\n",
    "            if title:\n",
    "                axes[i].set_title(title[i])\n",
    "            axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
    "                           interpolation='none', cmap='inferno')\n",
    "        axes[0].set(xlabel=\"Frames\", ylabel=\"Channels\")\n",
    "        axes[1].set(xlabel=\"Decoder timestep\", ylabel=\"Encoder timestep\")\n",
    "    else:\n",
    "        fig, axes = plt.subplots(len(data), 1, figsize=(int(alignment_graph_width*graph_scale/100), int(alignment_graph_height*graph_scale/100)//2))\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        axes.imshow(data[0], aspect='auto', origin='bottom', interpolation='none', cmap='inferno')\n",
    "        axes.set(xlabel=\"Frames\", ylabel=\"Channels\")\n",
    "    fig.canvas.draw()\n",
    "    plt.show()\n",
    "\n",
    "#dictionary_path = r\"/media/cookie/Samsung PM961/TwiBot/tacotron2/filelists/merged.dict_.txt\"\n",
    "dictionary_path = r\"G:\\TwiBot\\tacotron2\\filelists\\merged.dict_.txt\"\n",
    "print(\"Running, Please wait...\")\n",
    "thisdict = {}\n",
    "for line in reversed((open(dictionary_path, \"r\").read()).splitlines()):\n",
    "    thisdict[(line.split(\" \", 1))[0]] = (line.split(\" \", 1))[1].strip()\n",
    "print(\"Dictionary Ready.\")\n",
    "sym = list(\"☺☻♥♦♣♠•◘○◙♂♀♪♫☼►◄↕‼¶§▬↨↑↓→←∟↔▲▼\")\n",
    "def ARPA(text_, punc=r\"!?,.;:␤#-_'\\\"()[]\"):\n",
    "    text = text_.replace(\"\\n\",\" \"); out = ''\n",
    "    for word_ in text.split(\" \"):\n",
    "        word=word_; end_chars = ''; start_chars = ''\n",
    "        while any(elem in word for elem in punc) and len(word) > 1:\n",
    "            if word[-1] in punc: end_chars = word[-1] + end_chars; word = word[:-1]\n",
    "            elif word[0] in punc: start_chars = start_chars + word[0]; word = word[1:]\n",
    "            else: break\n",
    "        try: word_arpa = thisdict[word.upper()]\n",
    "        except: word_arpa = ''\n",
    "        if len(word_arpa)!=0: word = \"{\" + str(word_arpa) + \"}\"\n",
    "        out = (out + \" \" + start_chars + word + end_chars).strip()\n",
    "    #if out[-1] != \"␤\": out = out + \"␤\"\n",
    "    #if out[0] != \"☺\": out = \"☺\" + out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "epochs=1000,iters_per_checkpoint=1000,iters_per_validation=1000,seed=1234,dynamic_loss_scaling=True,fp16_run=False,distributed_run=False,dist_backend=nccl,dist_url=tcp://127.0.0.1:54321,cudnn_enabled=True,cudnn_benchmark=False,ignore_layers=[],frozen_layers=[0],load_mel_from_disk=True,speakerlist=/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/speaker_ids.txt,training_files=/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/mel_train_taca2_merged.txt,validation_files=/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/mel_validation_taca2_merged.txt,text_cleaners=['basic_cleaners'],max_wav_value=32768.0,sampling_rate=48000,filter_length=2400,hop_length=600,win_length=2400,n_mel_channels=160,mel_fmin=0.0,mel_fmax=16000.0,n_symbols=179,symbols_embedding_dim=512,gate_positive_weight=10,gate_threshold=0.6,gate_delay=10,max_decoder_steps=1000,low_vram_inference=False,p_teacher_forcing=1.0,teacher_force_till=20,val_p_teacher_forcing=0.8,val_teacher_force_till=20,encoder_speaker_embed_dim=64,encoder_concat_speaker_embed=before_conv,encoder_kernel_size=5,encoder_n_convolutions=3,encoder_conv_hidden_dim=512,encoder_LSTM_dim=768,start_token=,stop_token=,hide_startstop_tokens=False,n_frames_per_step=1,context_frames=1,prenet_dim=512,prenet_layers=2,prenet_batchnorm=False,p_prenet_dropout=0.5,prenet_speaker_embed_dim=0,attention_rnn_dim=1280,AttRNN_extra_decoder_input=True,AttRNN_hidden_dropout_type=zoneout,p_AttRNN_hidden_dropout=0.1,p_AttRNN_cell_dropout=0.0,n_speakers=512,speaker_embedding_dim=256,decoder_rnn_dim=1792,extra_projection=False,DecRNN_hidden_dropout_type=zoneout,p_DecRNN_hidden_dropout=0.2,p_DecRNN_cell_dropout=0.0,attention_type=0,attention_dim=128,attention_location_n_filters=32,attention_location_kernel_size=31,num_att_mixtures=1,attention_layers=1,delta_offset=0,delta_min_limit=0,lin_bias=False,initial_gain=relu,normalize_attention_input=True,normalize_AttRNN_output=False,postnet_embedding_dim=512,postnet_kernel_size=5,postnet_n_convolutions=5,with_gst=True,ref_enc_pack_padded_seq=True,ref_enc_filters=[32, 32, 64, 64, 128, 128],ref_enc_size=[3, 3],ref_enc_strides=[2, 2],ref_enc_pad=[1, 1],ref_enc_gru_size=128,gstAtt_dim=128,num_heads=8,token_num=5,token_activation_func=tanh,token_embedding_size=256,torchMoji_attDim=2304,torchMoji_linear=True,torchMoji_training=True,p_drop_tokens=0.0,drop_tokens_mode=zeros,use_saved_learning_rate=False,loss_func=MSELoss,learning_rate=1e-06,weight_decay=1e-06,grad_clip_thresh=1.0,batch_size=28,val_batch_size=28,truncated_length=640,mask_padding=True,global_mean_npy=global_mean.npy,drop_frame_rate=0.25\n"
     ]
    }
   ],
   "source": [
    "hparams = create_hparams()\n",
    "hparams.max_decoder_steps = 1000\n",
    "hparams.gate_threshold = 0.6\n",
    "hparams.ignore_layers = []\n",
    "print(str(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows WaveGlow from Ground Truth\n",
    "from utils import load_wav_to_torch\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate, max_value = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / max(max_value, audio.max(), -audio.min())\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Tacotron2 model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tacotron... Done\n",
      "This Tacotron model has been trained for  53000  Iterations.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_53000\"\n",
    "print(\"Loading Tacotron... \", end=\"\")\n",
    "checkpoint_hparams = torch.load(checkpoint_path)['hparams']\n",
    "#checkpoint_hparams.parse_json(hparams.to_json())\n",
    "model = load_model(checkpoint_hparams)\n",
    "#checkpoint_dict = {k.replace(\"encoder_speaker_embedding.weight\",\"encoder.encoder_speaker_embedding.weight\"): v for k,v in torch.load(checkpoint_path)['state_dict'].items()}\n",
    "checkpoint_dict = torch.load(checkpoint_path)['state_dict']\n",
    "model.load_state_dict(checkpoint_dict)\n",
    "_ = model.cuda().eval().half(); print(\"Done\")\n",
    "\n",
    "tacotron_speaker_id_lookup = torch.load(checkpoint_path)['speaker_id_lookup']\n",
    "print(\"This Tacotron model has been trained for \",torch.load(checkpoint_path)['iteration'],\" Iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resave Checkpoint without optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "checkpoint_path = r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_178000\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "checkpoint['optimizer'] = None\n",
    "\n",
    "fpath = f\"{checkpoint_path}_weights\"\n",
    "if  not exists(fpath):\n",
    "    torch.save(checkpoint, fpath)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Already Exists! Skipping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decoder.prenet.layers.0.linear_layer.weight',\n",
       " 'decoder.prenet.layers.1.linear_layer.weight',\n",
       " 'decoder.attention_rnn.weight_ih',\n",
       " 'decoder.attention_rnn.weight_hh',\n",
       " 'decoder.attention_rnn.bias_ih',\n",
       " 'decoder.attention_rnn.bias_hh',\n",
       " 'decoder.attention_layer.query_layer.linear_layer.weight',\n",
       " 'decoder.attention_layer.memory_layer.linear_layer.weight',\n",
       " 'decoder.attention_layer.v.linear_layer.weight',\n",
       " 'decoder.attention_layer.location_layer.location_conv.conv.weight',\n",
       " 'decoder.attention_layer.location_layer.location_dense.linear_layer.weight',\n",
       " 'decoder.decoder_rnn.weight_ih',\n",
       " 'decoder.decoder_rnn.weight_hh',\n",
       " 'decoder.decoder_rnn.bias_ih',\n",
       " 'decoder.decoder_rnn.bias_hh',\n",
       " 'decoder.linear_projection.linear_layer.weight',\n",
       " 'decoder.linear_projection.linear_layer.bias',\n",
       " 'decoder.gate_layer.linear_layer.weight',\n",
       " 'decoder.gate_layer.linear_layer.bias']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in list(model.named_parameters()) if x[0].startswith(\"decoder\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load WaveGlow from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yoyo': True, 'yoyo_WN': False, 'n_mel_channels': 160, 'n_flows': 6, 'n_group': 24, 'n_early_every': 6, 'n_early_size': 2, 'memory_efficient': False, 'spect_scaling': False, 'upsample_mode': 'normal', 'WN_config': {'n_layers': 8, 'n_channels': 384, 'kernel_size': 3, 'speaker_embed_dim': 96, 'rezero': False}, 'win_length': 2400, 'hop_length': 600}\n",
      "Config File from 'H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing5\\config.json' successfully loaded.\n",
      "intializing WaveGlow model... Done!\n",
      "loading WaveGlow checkpoint... Done!\n",
      "initializing Denoiser... Done!\n",
      "359500 iterations\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "waveglow_path = r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing5\\best_val_model\" #r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing2\\best_val_model\"\n",
    "config_fpath =  r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing5\\config.json\" #r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing2\\config.json\"\n",
    "\n",
    "# Load config file\n",
    "with open(config_fpath) as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "train_config = config[\"train_config\"]\n",
    "global data_config\n",
    "data_config = config[\"data_config\"]\n",
    "global dist_config\n",
    "dist_config = config[\"dist_config\"]\n",
    "global waveglow_config\n",
    "waveglow_config = {\n",
    "    **config[\"waveglow_config\"], \n",
    "    'win_length': data_config['win_length'],\n",
    "    'hop_length': data_config['hop_length']\n",
    "}\n",
    "print(waveglow_config)\n",
    "print(f\"Config File from '{config_fpath}' successfully loaded.\")\n",
    "\n",
    "# import the correct model\n",
    "if waveglow_config[\"yoyo\"]: # efficient_mode # TODO: Add to Config File\n",
    "    from efficient_model import WaveGlow\n",
    "else:\n",
    "    from glow import WaveGlow\n",
    "\n",
    "# initialize model\n",
    "print(f\"intializing WaveGlow model... \", end=\"\")\n",
    "waveglow = WaveGlow(**waveglow_config).cuda()\n",
    "print(f\"Done!\")\n",
    "\n",
    "# load checkpoint from file\n",
    "print(f\"loading WaveGlow checkpoint... \", end=\"\")\n",
    "checkpoint = torch.load(waveglow_path)\n",
    "waveglow.load_state_dict(checkpoint['model']) # and overwrite initialized weights with checkpointed weights\n",
    "waveglow.cuda().eval().half() # move to GPU and convert to half precision\n",
    "print(f\"Done!\")\n",
    "#for k in waveglow.convinv:\n",
    "#    k.float()\n",
    "print(f\"initializing Denoiser... \", end=\"\")\n",
    "denoiser = Denoiser(waveglow)\n",
    "print(f\"Done!\")\n",
    "waveglow_iters = torch.load(waveglow_path)['iteration']\n",
    "print(waveglow_iters, \"iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resave Checkpoint without optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "checkpoint['optimizer'] = None\n",
    "\n",
    "fpath = waveglow_path.replace(r\"\\best_val_model\",r\"\\best_val_weights\")\n",
    "if  not exists(fpath):\n",
    "    torch.save(checkpoint, fpath)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Already Exists! Skipping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x = [0,1,2,3,4,5,6,7]\n",
    "for i in range(20):\n",
    "    print(x[i%len(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_names = ['one','two','three']\n",
    "def shuffle_and_return():\n",
    "    speaker_names.append(speaker_names.pop(0))\n",
    "    return speaker_names[0]\n",
    "simultaneous_texts = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'three']\n"
     ]
    }
   ],
   "source": [
    "batch_speaker_names = [shuffle_and_return() for i in range(simultaneous_texts)]\n",
    "print(batch_speaker_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 1) Get Speaker ID's from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tacotron_speaker_id_lookup\n",
    "waveglow_speaker_id_lookup = checkpoint['speaker_lookup']\n",
    "print(str(waveglow_speaker_id_lookup).replace(\",\",\"\\n\").replace(\":\",\" ->\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 2) Rebuild Speaker ID's from training filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_utils import TextMelLoader\n",
    "#from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#speaker_ids = TextMelLoader(\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/mel_train_taca2.txt\", hparams).speaker_ids\n",
    "#speaker_ids = TextMelLoader(r\"D:\\ClipperDatasetV2/filelists/mel_train_taca2.txt\", hparams, check_files=False, TBPTT=False).speaker_ids\n",
    "#print(str(speaker_ids).replace(\", \",\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load TorchMoji for Style Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Use torchMoji to score texts for emoji distribution.\n",
    "\n",
    "The resulting emoji ids (0-63) correspond to the mapping\n",
    "in emoji_overview.png file at the root of the torchMoji repo.\n",
    "\n",
    "Writes the result to a csv file.\n",
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "maxlen = 180\n",
    "texts = [\"Testing!\",]\n",
    "\n",
    "with torch.no_grad():\n",
    "    st = SentenceTokenizer(vocabulary, maxlen, ignore_sentences_with_only_custom=True)\n",
    "    torchmoji = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "    tokenized, _, _ = st.tokenize_sentences(texts) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "    embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "    print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a measure for Alignment quality in inferred clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    if not max_len:\n",
    "        max_len = torch.max(lengths).long()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device, dtype=torch.int64)\n",
    "    mask = (ids < lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "# New MUCH more performant version, (doesn't support unique padded inputs, just iterate over the batch dim or smthn if you need padded inputs cause this is still way faster)\n",
    "# @torch.jit.script # should work and be even faster, but makes it harder to debug and it's already fast enough right now\n",
    "def alignment_metric(alignments, input_lengths=None, output_lengths=None, average_across_batch=False):\n",
    "    alignments = alignments.transpose(1,2) # [B, dec, enc] -> [B, enc, dec]\n",
    "    # alignments [batch size, x, y]\n",
    "    # input_lengths [batch size] for len_x\n",
    "    # output_lengths [batch size] for len_y\n",
    "    if input_lengths == None:\n",
    "        input_lengths =  torch.ones(alignments.size(0), device=alignments.device)*(alignments.shape[1]-1) # [B] # 147\n",
    "    if output_lengths == None:\n",
    "        output_lengths = torch.ones(alignments.size(0), device=alignments.device)*(alignments.shape[2]-1) # [B] # 767\n",
    "    batch_size = alignments.size(0)\n",
    "    optimums = torch.sqrt(input_lengths.double()**2 + output_lengths.double()**2).view(batch_size)\n",
    "    \n",
    "    # [B, enc, dec] -> [B, dec], [B, dec]\n",
    "    values, cur_idxs = torch.max(alignments, 1) # get max value in column and location of max value\n",
    "    \n",
    "    cur_idxs = cur_idxs.float()\n",
    "    prev_indx = torch.cat((cur_idxs[:,0][:,None], cur_idxs[:,:-1]), dim=1) # shift entire tensor right by one.\n",
    "    dist = ((prev_indx - cur_idxs).pow(2) + 1).pow(0.5)\n",
    "    dist.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=dist.size(1)), 0.0) # remove padding\n",
    "    dist = dist.sum(dim=(1)) # remove padding\n",
    "    diagonalitys = (dist + 1.4142135)/optimums # remove padding\n",
    "    \n",
    "    alignments.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=alignments.size(2))[:,None,:], 0.0)\n",
    "    encoder_max_focus = torch.sum(alignments, dim=2).max(dim=1)[0] # [B, enc, dec] -> [B, sum_enc] -> [B]\n",
    "    encoder_min_focus = torch.sum(alignments, dim=2).min(dim=1)[0]\n",
    "    encoder_avg_focus = torch.sum(alignments, dim=2).mean(dim=1)\n",
    "    \n",
    "    values.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=values.size(1)), 0.0) # because padding\n",
    "    avg_prob = values.mean(dim=1)\n",
    "    avg_prob *= (alignments.size(2)/output_lengths.float()) # because padding\n",
    "    \n",
    "    if average_across_batch:\n",
    "        diagonalitys = diagonalitys.mean()\n",
    "        encoder_max_focus = encoder_max_focus.mean()\n",
    "        encoder_min_focus = encoder_min_focus.mean()\n",
    "        encoder_avg_focus = encoder_avg_focus.mean()\n",
    "        avg_prob = avg_prob.mean()\n",
    "    return diagonalitys.cpu(), avg_prob.cpu(), encoder_max_focus.cpu(), encoder_min_focus.cpu(), encoder_avg_focus.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesize audio (From Filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from time import sleep\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\"\"\"\n",
    "|(Game) Them's Fightin' Herds_Oleander|150\n",
    "|(Game) Them's Fightin' Herds_Fred|151\n",
    "|(Game) Them's Fightin' Herds_Pom|152\n",
    "|(Game) Them's Fightin' Herds_Velvet|153\n",
    "|(Game) Them's Fightin' Herds_Arizona|154\n",
    "|(Game) Them's Fightin' Herds_Tianhuo|155\n",
    "|(Game) Elite Dangerous_Eli|156\n",
    "|(Audiobook) A Little Bit Wicked_Skystar|157\n",
    "|(Audiobook) Dr. Who_Doctor|158\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "|(Show) My Little Pony_Rainbow|160\n",
    "|(Show) My Little Pony_Pinkie|161\n",
    "|(Show) My Little Pony_Rarity|162\n",
    "|(Show) My Little Pony_Spike|163\n",
    "|(Show) My Little Pony_Fluttershy|164\n",
    "|(Show) My Little Pony_Nightmare Moon|165\n",
    "|(Show) Dan Vs_Dan|166\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "|(Show) My Little Pony_Scootaloo|168\n",
    "|(Show) My Little Pony_Big Mac|169\n",
    "|(Show) My Little Pony_Sweetie Belle|170\n",
    "|(Show) My Little Pony_Apple Bloom|171\n",
    "\"\"\"\n",
    "\n",
    "speakers = \"\"\"\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "narrators = \"\"\"\n",
    "|(Audiodrama) Fallout Equestria_Littlepip|1\n",
    "|(Game) Them's Fightin' Herds_Oleander|150\n",
    "|(Game) Them's Fightin' Herds_Pom|152\n",
    "|(Game) Them's Fightin' Herds_Velvet|153\n",
    "|(Game) Them's Fightin' Herds_Arizona|154\n",
    "|(Game) Them's Fightin' Herds_Tianhuo|155\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "|(Show) My Little Pony_Rainbow|160\n",
    "|(Show) My Little Pony_Pinkie|161\n",
    "|(Show) My Little Pony_Rarity|162\n",
    "|(Show) My Little Pony_Spike|163\n",
    "|(Show) My Little Pony_Fluttershy|164\n",
    "|(Show) Dan Vs_Dan|166\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "|(Show) My Little Pony_Scootaloo|168\n",
    "|(Show) My Little Pony_Sweetie Belle|170\n",
    "|(Show) My Little Pony_Apple Bloom|171\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "texts = \"\"\"\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmm, that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmm, that feels nice.\n",
    "Mmmmmmmmmmmmmmm, that feels nice.\n",
    "Mmmmmmmmmmmm that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm that feels nice.\n",
    "\"\"\"\n",
    "_audio_path_override = None\n",
    "_speaker_id_override = None\n",
    "style_mode = 'torchmoji_hidden' # Options = 'mel','token','zeros','torchmoji_hidden','torchmoji_string'\n",
    "\n",
    "acceptable_score = 0.8 # sufficient score to just skip ahead instead of checking/generating more outputs\n",
    "absolutely_required_score = 0.3 # retry forever until this score is reached\n",
    "absolute_maximum_tries = 256 # this is per text text input\n",
    "\n",
    "# Score Parameters\n",
    "diagonality_weighting = 0.5 # 'stutter factor', a penalty for clips where the model jumps back and forwards in the sentence.\n",
    "max_focus_weighting = 1.0   # 'stuck factor', a penalty for clips that spend execisve time on the same letter.\n",
    "min_focus_weighting = 1.0   # 'miniskip factor', a penalty for skipping/ignoring small parts of the input text.\n",
    "avg_focus_weighting = 1.0   # 'skip factor', a penalty for skipping very large parts of the input text\n",
    "max_attempts = 128 # retries at each clip # this is per text input\n",
    "batch_size_per_text = 128 # minibatch_size per unique text input\n",
    "simultaneous_texts = 1 # num unique text inputs per batch\n",
    "\n",
    "max_decoder_steps = 1600\n",
    "max_text_segment_length = 120\n",
    "gate_threshold = 0.7\n",
    "gate_delay = 3\n",
    "use_arpabet = 1\n",
    "\n",
    "sigma = 0.95\n",
    "audio_save_path = r\"D:\\Downloads\\infer\\testing\"\n",
    "output_filename = 'TestingOutput'\n",
    "save_wavs = 1 # saves wavs to infer folder\n",
    "\n",
    "show_all_attempt_scores = 0\n",
    "show_audio_overwrite_warnings = 1\n",
    "show_input_text = 1\n",
    "show_best_score = 1\n",
    "show_audio  = 1\n",
    "show_graphs_tacotron = 1\n",
    "show_graphs_waveglow = 1\n",
    "status_updates = 1 # ... Done\n",
    "time_to_gen = 1\n",
    "graph_scale = 0.5\n",
    "alignment_graph_width = 3840\n",
    "alignment_graph_height = 1920\n",
    "\n",
    "model.decoder.gate_delay = gate_delay\n",
    "model.decoder.max_decoder_steps = max_decoder_steps\n",
    "model.decoder.gate_threshold = gate_threshold\n",
    "\n",
    "os.makedirs(audio_save_path, exist_ok=True)\n",
    "texts_segmented = [x for x in texts.split(\"\\n\") if len(x.strip())]\n",
    "total_len = len(texts_segmented)\n",
    "\n",
    "continue_from = 0 # skip\n",
    "counter = 0\n",
    "text_batch_in_progress = []\n",
    "for text_index, text in enumerate(texts_segmented):\n",
    "    if text_index < continue_from: print(f\"Skipping {text_index}.\\t\",end=\"\"); counter+=1; continue\n",
    "    print(f\"{text_index}/{total_len}|{datetime.now()}\")\n",
    "    \n",
    "    # setup the text batches\n",
    "    text_batch_in_progress.append(text)\n",
    "    if (len(text_batch_in_progress) == simultaneous_texts) or (text_index == (len(texts_segmented)-1)): # if text batch ready or final input\n",
    "        text_batch = text_batch_in_progress\n",
    "        text_batch_in_progress = []\n",
    "        if (text_index == (len(texts_segmented)-1)): # if final text input\n",
    "            simultaneous_texts = len(text_batch) # ensure batch size is still correct\n",
    "    else:\n",
    "        continue # if batch not ready, add another text\n",
    "    \n",
    "    # pick the speakers for the texts\n",
    "    speaker_ids = [random.choice(speakers).split(\"|\")[2] if ('\"' in text) else random.choice(narrators).split(\"|\")[2] for text in text_batch] # pick speaker if quotemark in text, else narrator\n",
    "    text_batch  = [text.replace('\"',\"\") for text in text_batch] # remove quotes from text\n",
    "    \n",
    "    if _audio_path_override != None:\n",
    "        audio_path = _audio_path_override\n",
    "    if _speaker_id_override != None:\n",
    "        speaker_id = _speaker_id_override\n",
    "    \n",
    "    # get speaker_ids (tacotron)\n",
    "    tacotron_speaker_ids = [tacotron_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    tacotron_speaker_ids = torch.LongTensor(tacotron_speaker_ids).cuda().repeat_interleave(batch_size_per_text)\n",
    "    \n",
    "    # get speaker_ids (waveglow)\n",
    "    waveglow_speaker_ids = [waveglow_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    waveglow_speaker_ids = torch.LongTensor(waveglow_speaker_ids).cuda()\n",
    "    \n",
    "    # style\n",
    "    if style_mode == 'mel':\n",
    "        mel = load_mel(audio_path.replace(\".npy\",\".wav\")).cuda().half()\n",
    "        style_input = mel\n",
    "    elif style_mode == 'token':\n",
    "        pass\n",
    "        #style_input =\n",
    "    elif style_mode == 'zeros':\n",
    "        style_input = None\n",
    "    elif style_mode == 'torchmoji_hidden':\n",
    "        try:\n",
    "            tokenized, _, _ = st.tokenize_sentences(text_batch) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "        except:\n",
    "            raise Exception(f\"text\\n{text_batch}\\nfailed to tokenize.\")\n",
    "        try:\n",
    "            embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "        except Exception as ex:\n",
    "            print(f'Exception: {ex}')\n",
    "            print(f\"text: {text_batch} failed to process.\")\n",
    "            #raise Exception(f\"text\\n{text}\\nfailed to process.\")\n",
    "        style_input = torch.from_numpy(embedding).cuda().half().repeat_interleave(batch_size_per_text, dim=0)\n",
    "    elif style_mode == 'torchmoji_string':\n",
    "        style_input = text_batch\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # check punctuation\n",
    "    valid_last_char = '-,.?!;:' # valid final characters in texts\n",
    "    text_batch = [text+'.' if (text[-1] not in ',.?!;:') else text for text in text_batch]\n",
    "    \n",
    "    # parse text\n",
    "    text_batch = [unidecode(text.replace(\"...\",\". \").replace(\"  \",\" \").strip()) for text in text_batch] # remove eclipses, double spaces, unicode and spaces before/after the text.\n",
    "    if show_input_text: # debug\n",
    "        print(\"raw_text:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    if use_arpabet: # convert texts to ARPAbet (phonetic) versions.\n",
    "        text_batch = [ARPA(text) for text in text_batch]\n",
    "    if show_input_text: # debug\n",
    "        print(\"model_input:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if time_to_gen:\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # convert texts to sequence, pad where appropriate and move to GPU\n",
    "        sequence_split = [torch.LongTensor(text_to_sequence(text, ['english_cleaners'])) for text in text_batch] # convert texts to numpy representation\n",
    "        text_lengths = torch.tensor([seq.size(0) for seq in sequence_split])\n",
    "        max_len = text_lengths.max().item()\n",
    "        sequence = torch.zeros(text_lengths.size(0), max_len).long() # create large tensor to move each text input into\n",
    "        for i in range(text_lengths.size(0)): # move each text into padded input tensor\n",
    "            sequence[i, :sequence_split[i].size(0)] = sequence_split[i]\n",
    "        sequence = sequence.cuda().long().repeat_interleave(batch_size_per_text, dim=0) # move to GPU and repeat text\n",
    "        text_lengths = text_lengths.cuda().long() # move to GPU\n",
    "        #print(\"max_len =\", max_len) # debug\n",
    "        #print( get_mask_from_lengths(text_lengths).shape ) # debug\n",
    "        #sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long().repeat_interleave(batch_size_per_text, 0)# convert numpy to tensor and repeat for each text\n",
    "        \n",
    "        # debug\n",
    "        text_lengths = text_lengths.clone()\n",
    "        sequence = sequence.clone()\n",
    "        \n",
    "        for i in range(1):\n",
    "            try:\n",
    "                best_score = np.ones(simultaneous_texts) * -9e9\n",
    "                tries      = np.zeros(simultaneous_texts)\n",
    "                best_generations = [0]*simultaneous_texts\n",
    "                best_score_str = ['']*simultaneous_texts\n",
    "                while np.amin(best_score) < acceptable_score:\n",
    "                    # run inference\n",
    "                    if status_updates: print(\"Running Tacotron2... \", end='')\n",
    "                    mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = model.inference(sequence, tacotron_speaker_ids, style_input=style_input, style_mode=style_mode, text_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0))\n",
    "                    \n",
    "                    # find metrics for each item\n",
    "                    gate_batch_outputs[:,:20] = 0 # ignore gate predictions for the first bit\n",
    "                    output_lengths = gate_batch_outputs.argmax(dim=1)+gate_delay\n",
    "                    diagonality_batch, avg_prob_batch, enc_max_focus_batch, enc_min_focus_batch, enc_avg_focus_batch = alignment_metric(alignments_batch, input_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0), output_lengths=output_lengths)\n",
    "                    \n",
    "                    batch = list(zip(\n",
    "                        mel_batch_outputs.split(1,dim=0),\n",
    "                        mel_batch_outputs_postnet.split(1,dim=0),\n",
    "                        gate_batch_outputs.split(1,dim=0),\n",
    "                        alignments_batch.split(1,dim=0),\n",
    "                        diagonality_batch,\n",
    "                        avg_prob_batch,\n",
    "                        enc_max_focus_batch,\n",
    "                        enc_min_focus_batch,\n",
    "                        enc_avg_focus_batch,))\n",
    "                    # split batch into items\n",
    "                    \n",
    "                    for j in range(simultaneous_texts): # process each set of text spectrograms seperately\n",
    "                        start, end = (j*batch_size_per_text), ((j+1)*batch_size_per_text)\n",
    "                        sametext_batch = batch[start:end] # seperate the full batch into pieces that use the same input text\n",
    "                        \n",
    "                        # process all items related to the j'th text input\n",
    "                        for k, (mel_outputs, mel_outputs_postnet, gate_outputs, alignments, diagonality, avg_prob, enc_max_focus, enc_min_focus, enc_avg_focus) in enumerate(sametext_batch):\n",
    "                            # factors that make up score\n",
    "                            weighted_score =  avg_prob.item() # general alignment quality\n",
    "                            weighted_score -= (max(diagonality.item(),1.11)-1.11) * diagonality_weighting  # consistent pace\n",
    "                            weighted_score -= max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting # getting stuck on pauses/phones\n",
    "                            weighted_score -= max(0.9-enc_min_focus.item(),0) * min_focus_weighting # skipping single enc outputs\n",
    "                            weighted_score -= max(2.5-enc_avg_focus.item(), 0) * avg_focus_weighting # skipping most enc outputs\n",
    "                            score_str = f\"{round(diagonality.item(),3)} {round(avg_prob.item()*100,2)}% {round(weighted_score,4)} {round(max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting,2)} {round(max(0.9-enc_min_focus.item(),0),2)}|\"\n",
    "                            if weighted_score > best_score[j]:\n",
    "                                best_score[j] = weighted_score\n",
    "                                best_score_str[j] = score_str\n",
    "                                best_generations[j] = [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "                            if show_all_attempt_scores:\n",
    "                                print(score_str, end=\"\")\n",
    "                            tries[j]+=1\n",
    "                            if np.amin(tries) >= max_attempts and np.amin(best_score) > (absolutely_required_score-1):\n",
    "                                raise StopIteration\n",
    "                            if np.amin(tries) >= absolute_maximum_tries:\n",
    "                                print(f\"Absolutely required score not achieved in {absolute_maximum_tries} attempts - \", end='')\n",
    "                                raise StopIteration\n",
    "                    \n",
    "                    if np.amin(tries) < (max_attempts-1):\n",
    "                        print('Acceptable alignment/diagonality not reached. Retrying.')\n",
    "                    elif np.amin(best_score) < absolutely_required_score:\n",
    "                        print('Score less than absolutely required score. Retrying extra.')\n",
    "            except StopIteration:\n",
    "                del batch\n",
    "                if status_updates: print(\"Done\")\n",
    "                pass\n",
    "            # [[mel, melpost, gate, align], [mel, melpost, gate, align], [mel, melpost, gate, align]] -> [[mel, mel, mel], [melpost, melpost, melpost], [gate, gate, gate], [align, align, align]]\n",
    "            \n",
    "            # zip is being weird so alternative used\n",
    "            mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = [x[0][0].T for x in best_generations], [x[1][0].T for x in best_generations], [x[2][0] for x in best_generations], [x[3][0] for x in best_generations] # pickup whatever was the best attempts\n",
    "            \n",
    "            # stack arrays into tensors\n",
    "            gate_batch_outputs = torch.nn.utils.rnn.pad_sequence(gate_batch_outputs, batch_first=True, padding_value=0)\n",
    "#            print(gate_batch_outputs.shape)\n",
    "            max_length = torch.max(gate_batch_outputs.argmax(dim=1)) # get highest duration\n",
    "            mel_batch_outputs = torch.nn.utils.rnn.pad_sequence(mel_batch_outputs, batch_first=True, padding_value=-11.6).transpose(1,2)[:,:,:max_length]\n",
    "            mel_batch_outputs_postnet = torch.nn.utils.rnn.pad_sequence(mel_batch_outputs_postnet, batch_first=True, padding_value=-11.6).transpose(1,2)[:,:,:max_length]\n",
    "            alignments_batch = torch.nn.utils.rnn.pad_sequence(alignments_batch, batch_first=True, padding_value=0)[:,:max_length,:]\n",
    "            \n",
    "            if status_updates: print(\"Running WaveGlow... \", end='')\n",
    "            audio_batch = waveglow.infer(mel_batch_outputs_postnet, speaker_ids=waveglow_speaker_ids, sigma=sigma)\n",
    "            audio_denoised_batch = denoiser(audio_batch, strength=0.0001)[:, 0]\n",
    "            if status_updates: print('Done')\n",
    "            \n",
    "            audio_len = 0\n",
    "            for j, (audio, audio_denoised) in enumerate(zip(audio_batch.split(1, dim=0), audio_denoised_batch.split(1, dim=0))):\n",
    "                # remove WaveGlow padding\n",
    "                audio_end = (gate_batch_outputs[j].argmax()+gate_delay) * hparams.hop_length\n",
    "                audio = audio[:,:audio_end]\n",
    "                audio_denoised = audio_denoised[:,:audio_end]\n",
    "                \n",
    "                # remove Tacotron2 padding\n",
    "                spec_end = gate_batch_outputs[j].argmax()+gate_delay\n",
    "                mel_outputs = mel_batch_outputs.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"mel_outputs.split(blah)[j].shape\", mel_outputs.shape)\n",
    "                mel_outputs_postnet = mel_batch_outputs_postnet.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"alignments_batch.shape\", alignments_batch.shape)\n",
    "                alignments = alignments_batch.split(1, dim=0)[j][:,:spec_end,:text_lengths[j]]\n",
    "                ##print(\"alignments.split(blah)[j].shape\", alignments.shape)\n",
    "                \n",
    "                if show_best_score:\n",
    "                    print(f\"Score: {round(best_score[j],3)}\\t\\tStats: {best_score_str[j]}  Verified: {[x.item() for x in alignment_metric(alignments)]}\")\n",
    "                if show_graphs_tacotron:\n",
    "                    plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
    "                           alignments.float().data.cpu().numpy()[0].T), title=[\"Spectrogram (Tacotron)\",\"Alignment (Tacotron)\"])\n",
    "                if show_audio:\n",
    "                        ipd.display(ipd.Audio(audio.cpu().numpy(), rate=hparams.sampling_rate))        \n",
    "                if save_wavs:\n",
    "                    save_audio_path = os.path.join(audio_save_path,f\"audio_{counter//300:02}_{counter:05}.wav\")\n",
    "                    if os.path.exists(save_audio_path):\n",
    "                        if show_audio_overwrite_warnings:\n",
    "                            print(f\"File already found at [{save_audio_path}], overwriting.\")\n",
    "                        os.remove(save_audio_path)\n",
    "                    if status_updates: print(f\"Saving clip to [{save_audio_path}]... \", end=\"\")\n",
    "                    librosa.output.write_wav(save_audio_path, np.swapaxes(audio.float().cpu().numpy(),0,1), hparams.sampling_rate)\n",
    "                    if status_updates: print(\"Done\")\n",
    "                if show_graphs_waveglow and save_wavs:\n",
    "                    plot_data([load_mel(save_audio_path).float().data.cpu().numpy()[0],], title = \"Spectrogram (WaveGlow)\")\n",
    "                counter+=1\n",
    "                audio_len+=audio_end\n",
    "            \n",
    "            if time_to_gen:\n",
    "                audio_seconds_generated = round(audio_len.item()/hparams.sampling_rate,3)\n",
    "                print(f\"Took {round(time.time()-start_time,3)}s to generate {audio_seconds_generated}s of audio. (best of {tries.sum().astype('int')} tries)\")\n",
    "            \n",
    "            print(\"\")\n",
    "_text = None; _audio_path = None; _speaker_id = None\n",
    "\n",
    "# Merge clips and output the concatenated result\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# get number of intermediate concatenations required (Sox can only merge 340~ files at a time)\n",
    "n_audio_batches = -(-len( glob(os.path.join(audio_save_path, \"audio_*_*.wav\")) ) // 300)\n",
    "\n",
    "# ensure path ends in .wav\n",
    "if not output_filename[-4:].lower() == '.wav':\n",
    "    output_filename+='.wav'\n",
    "\n",
    "for i in range(n_audio_batches):\n",
    "    print(f\"Merging audio files {i*300} to {((i+1)*300)-1}... \", end='')\n",
    "    os.system(f'sox {os.path.join(audio_save_path, f\"audio_{i:02}_*.wav\")} -b 16 {os.path.join(audio_save_path, f\"concat_{i:02}.wav\")}')\n",
    "    print(\"Done\")\n",
    "print(f\"Saving output to '{os.path.join(audio_save_path, output_filename)}'... \", end='')\n",
    "os.system(f'sox \"{os.path.join(audio_save_path, \"concat_*.wav\")}\" -b 16 \"{os.path.join(audio_save_path, output_filename)}\"') # merge the merged files into a final output. bit depth of 16 required to go over 4 hour length\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "x = [1,2,3]\n",
    "[i for i in x for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*3*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "48000/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users = [\"Twilight Sparkle\",\"Rainbow Dash\",\"Fluttershy\",\"Applejack\"]\n",
    "difflib.get_close_matches(\"TS\", valid_users, n=4, cutoff=0.01)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(0).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1,2,3,4,5,6,7][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesize audio (From Text List e.g Fimfic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    # list(chunks([0,1,2,3,4,5,6,7,8,9],2)) -> [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def parse_txt_into_quotes(fpath):\n",
    "    texts = open(fpath, \"r\", encoding=\"utf-8\").read()\n",
    "    \n",
    "    quo ='\"'; texts = [f'\"{text.replace(quo,\"\").strip()}\"' if i%2 else text.replace(quo,\"\").strip() for i, text in enumerate(unidecode(texts).split('\"'))]\n",
    "    \n",
    "    texts_segmented = []\n",
    "    for text in texts:\n",
    "        text = text.strip()\n",
    "        if not len(text.replace('\"','').strip()): continue\n",
    "        text = text\\\n",
    "            .replace(\"\\n\",\" \")\\\n",
    "            .replace(\"  \",\" \")\\\n",
    "            .replace(\"> --------------------------------------------------------------------------\",\"\")\n",
    "        if len(text) > max_text_segment_length:\n",
    "            for seg in [x.strip() for x in text.split(\".\") if len(x.strip()) if x is not '\"']: \n",
    "                if '\"' in text:\n",
    "                    if seg[0] != '\"': seg='\"'+seg\n",
    "                    if seg[-1] != '\"': seg+='\"'\n",
    "                texts_segmented.append(seg)\n",
    "        else:\n",
    "            texts_segmented.append(text.strip())\n",
    "    return texts_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from time import sleep\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\"\"\"\n",
    "|(Game) Them's Fightin' Herds_Oleander|150\n",
    "|(Game) Them's Fightin' Herds_Fred|151\n",
    "|(Game) Them's Fightin' Herds_Pom|152\n",
    "|(Game) Them's Fightin' Herds_Velvet|153\n",
    "|(Game) Them's Fightin' Herds_Arizona|154\n",
    "|(Game) Them's Fightin' Herds_Tianhuo|155\n",
    "|(Game) Elite Dangerous_Eli|156\n",
    "|(Audiobook) A Little Bit Wicked_Skystar|157\n",
    "|(Audiobook) Dr. Who_Doctor|158\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "|(Show) My Little Pony_Rainbow|160\n",
    "|(Show) My Little Pony_Pinkie|161\n",
    "|(Show) My Little Pony_Rarity|162\n",
    "|(Show) My Little Pony_Spike|163\n",
    "|(Show) My Little Pony_Fluttershy|164\n",
    "|(Show) My Little Pony_Nightmare Moon|165\n",
    "|(Show) Dan Vs_Dan|166\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "|(Show) My Little Pony_Scootaloo|168\n",
    "|(Show) My Little Pony_Big Mac|169\n",
    "|(Show) My Little Pony_Sweetie Belle|170\n",
    "|(Show) My Little Pony_Apple Bloom|171\n",
    "\"\"\"\n",
    "\n",
    "speakers = \"\"\"\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "narrators = \"\"\"\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "_audio_path_override = None\n",
    "_speaker_id_override = None\n",
    "style_mode = 'torchmoji_hidden' # Options = 'mel','token','zeros','torchmoji_hidden','torchmoji_string'\n",
    "\n",
    "acceptable_score = 0.8 # sufficient score to just skip ahead instead of checking/generating more outputs\n",
    "absolutely_required_score = 0.3 # retry forever until this score is reached\n",
    "absolute_maximum_tries = 512 # this is per text text input\n",
    "\n",
    "# Score Parameters\n",
    "diagonality_weighting = 0.5 # 'stutter factor', a penalty for clips where the model jumps back and forwards in the sentence.\n",
    "max_focus_weighting = 1.0   # 'stuck factor', a penalty for clips that spend execisve time on the same letter.\n",
    "min_focus_weighting = 1.0   # 'miniskip factor', a penalty for skipping/ignoring small parts of the input text.\n",
    "avg_focus_weighting = 1.0   # 'skip factor', a penalty for skipping very large parts of the input text\n",
    "max_attempts = 256 # retries at each clip # this is per text input\n",
    "batch_size_per_text = 256 # minibatch_size per unique text input\n",
    "simultaneous_texts = 1 # num unique text inputs per batch\n",
    "\n",
    "max_decoder_steps = 1600\n",
    "max_text_segment_length = 120\n",
    "gate_threshold = 0.7\n",
    "gate_delay = 3\n",
    "use_arpabet = 1\n",
    "\n",
    "sigma = 0.95\n",
    "audio_save_path = r\"D:\\Downloads\\infer\\audio\"\n",
    "output_filename = 'Mort Takes a Holiday'\n",
    "save_wavs = 1 # saves wavs to infer folder\n",
    "\n",
    "show_all_attempt_scores = 0\n",
    "show_audio_overwrite_warnings = 1\n",
    "show_input_text = 1\n",
    "show_best_score = 1\n",
    "show_audio  = 1\n",
    "show_graphs = 1\n",
    "status_updates = 1 # ... Done\n",
    "time_to_gen = 1\n",
    "graph_scale = 0.5\n",
    "alignment_graph_width = 3840\n",
    "alignment_graph_height = 1920\n",
    "\n",
    "model.decoder.gate_delay = gate_delay\n",
    "model.decoder.max_decoder_steps = max_decoder_steps\n",
    "model.decoder.gate_threshold = gate_threshold\n",
    "\n",
    "file_path = r\"D:\\Downloads\\infer\\text\\Mort Takes a Holiday.txt\"\n",
    "\n",
    "texts_segmented = parse_txt_into_quotes(file_path)\n",
    "\n",
    "total_len = len(texts_segmented)\n",
    "\n",
    "# 0 init\n",
    "# 1 append\n",
    "# 2 append, generate, blank\n",
    "# 1 append\n",
    "# 2 append, generate, blank\n",
    "# 1\n",
    "# 2\n",
    "\n",
    "continue_from = 2545 # skip\n",
    "counter = 0\n",
    "text_batch_in_progress = []\n",
    "for text_index, text in enumerate(texts_segmented):\n",
    "    if text_index < continue_from: print(f\"Skipping {text_index}.\\t\",end=\"\"); counter+=1; continue\n",
    "    print(f\"{text_index}/{total_len}|{datetime.now()}\")\n",
    "    \n",
    "    # setup the text batches\n",
    "    text_batch_in_progress.append(text)\n",
    "    if (len(text_batch_in_progress) == simultaneous_texts) or (text_index == (len(texts_segmented)-1)): # if text batch ready or final input\n",
    "        text_batch = text_batch_in_progress\n",
    "        text_batch_in_progress = []\n",
    "    else:\n",
    "        continue # if batch not ready, add another text\n",
    "    \n",
    "    # pick the speakers for the texts\n",
    "    speaker_ids = [random.choice(speakers).split(\"|\")[2] if ('\"' in text) else random.choice(narrators).split(\"|\")[2] for text in text_batch] # pick speaker if quotemark in text, else narrator\n",
    "    text_batch  = [text.replace('\"',\"\") for text in text_batch] # remove quotes from text\n",
    "    \n",
    "    if _audio_path_override != None:\n",
    "        audio_path = _audio_path_override\n",
    "    if _speaker_id_override != None:\n",
    "        speaker_id = _speaker_id_override\n",
    "    \n",
    "    # get speaker_ids (tacotron)\n",
    "    tacotron_speaker_ids = [tacotron_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    tacotron_speaker_ids = torch.LongTensor(tacotron_speaker_ids).cuda().repeat_interleave(batch_size_per_text)\n",
    "    \n",
    "    # get speaker_ids (waveglow)\n",
    "    waveglow_speaker_ids = [waveglow_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    waveglow_speaker_ids = torch.LongTensor(waveglow_speaker_ids).cuda()\n",
    "    \n",
    "    # style\n",
    "    if style_mode == 'mel':\n",
    "        mel = load_mel(audio_path.replace(\".npy\",\".wav\")).cuda().half()\n",
    "        style_input = mel\n",
    "    elif style_mode == 'token':\n",
    "        pass\n",
    "        #style_input =\n",
    "    elif style_mode == 'zeros':\n",
    "        style_input = None\n",
    "    elif style_mode == 'torchmoji_hidden':\n",
    "        try:\n",
    "            tokenized, _, _ = st.tokenize_sentences(text_batch) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "        except:\n",
    "            raise Exception(f\"text\\n{text_batch}\\nfailed to tokenize.\")\n",
    "        try:\n",
    "            embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "        except Exception as ex:\n",
    "            print(f'Exception: {ex}')\n",
    "            print(f\"text: '{text_batch}' failed to process.\")\n",
    "            #raise Exception(f\"text\\n{text}\\nfailed to process.\")\n",
    "        style_input = torch.from_numpy(embedding).cuda().half().repeat_interleave(batch_size_per_text, dim=0)\n",
    "    elif style_mode == 'torchmoji_string':\n",
    "        style_input = text_batch\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # check punctuation\n",
    "    valid_last_char = '-,.?!;:' # valid final characters in texts\n",
    "    text_batch = [text+'.' if (text[-1] not in ',.?!;:') else text for text in text_batch]\n",
    "    \n",
    "    # parse text\n",
    "    text_batch = [unidecode(text.replace(\"...\",\". \").replace(\"  \",\" \").strip()) for text in text_batch] # remove eclipses, double spaces, unicode and spaces before/after the text.\n",
    "    if show_input_text: # debug\n",
    "        print(\"raw_text:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    if use_arpabet: # convert texts to ARPAbet (phonetic) versions.\n",
    "        text_batch = [ARPA(text) for text in text_batch]\n",
    "    if show_input_text: # debug\n",
    "        print(\"model_input:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if time_to_gen:\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # convert texts to sequence, pad where appropriate and move to GPU\n",
    "        sequence_split = [torch.LongTensor(text_to_sequence(text, ['english_cleaners'])) for text in text_batch] # convert texts to numpy representation\n",
    "        text_lengths = torch.tensor([seq.size(0) for seq in sequence_split])\n",
    "        max_len = text_lengths.max().item()\n",
    "        sequence = torch.zeros(text_lengths.size(0), max_len).long() # create large tensor to move each text input into\n",
    "        for i in range(text_lengths.size(0)): # move each text into padded input tensor\n",
    "            sequence[i, :sequence_split[i].size(0)] = sequence_split[i]\n",
    "        sequence = sequence.cuda().long().repeat_interleave(batch_size_per_text, dim=0) # move to GPU and repeat text\n",
    "        text_lengths = text_lengths.cuda().long() # move to GPU\n",
    "        #print(\"max_len =\", max_len) # debug\n",
    "        #print( get_mask_from_lengths(text_lengths).shape ) # debug\n",
    "        #sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long().repeat_interleave(batch_size_per_text, 0)# convert numpy to tensor and repeat for each text\n",
    "        \n",
    "        # debug\n",
    "        text_lengths = text_lengths.clone()\n",
    "        sequence = sequence.clone()\n",
    "        \n",
    "        for i in range(1):\n",
    "            try:\n",
    "                best_score = np.ones(simultaneous_texts) * -9e9\n",
    "                tries      = np.zeros(simultaneous_texts)\n",
    "                best_generations = [0]*simultaneous_texts\n",
    "                best_score_str = ['']*simultaneous_texts\n",
    "                while np.amin(best_score) < acceptable_score:\n",
    "                    # run inference\n",
    "                    if status_updates: print(\"Running Tacotron2... \", end='')\n",
    "                    mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = model.inference(sequence, tacotron_speaker_ids, style_input=style_input, style_mode=style_mode, text_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0))\n",
    "                    \n",
    "                    # find metrics for each item\n",
    "                    gate_batch_outputs[:,:20] = 0 # ignore gate predictions for the first 0.05s\n",
    "                    output_lengths = gate_batch_outputs.argmax(dim=1)+gate_delay\n",
    "                    diagonality_batch, avg_prob_batch, enc_max_focus_batch, enc_min_focus_batch, enc_avg_focus_batch = alignment_metric(alignments_batch, input_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0), output_lengths=output_lengths)\n",
    "                    \n",
    "                    batch = list(zip(\n",
    "                        mel_batch_outputs.split(1,dim=0),\n",
    "                        mel_batch_outputs_postnet.split(1,dim=0),\n",
    "                        gate_batch_outputs.split(1,dim=0),\n",
    "                        alignments_batch.split(1,dim=0),\n",
    "                        diagonality_batch,\n",
    "                        avg_prob_batch,\n",
    "                        enc_max_focus_batch,\n",
    "                        enc_min_focus_batch,\n",
    "                        enc_avg_focus_batch,))\n",
    "                    # split batch into items\n",
    "                    \n",
    "                    for j in range(simultaneous_texts): # process each set of text spectrograms seperately\n",
    "                        start, end = (j*batch_size_per_text), ((j+1)*batch_size_per_text)\n",
    "                        sametext_batch = batch[start:end] # seperate the full batch into pieces that use the same input text\n",
    "                        \n",
    "                        # process all items related to the j'th text input\n",
    "                        for k, (mel_outputs, mel_outputs_postnet, gate_outputs, alignments, diagonality, avg_prob, enc_max_focus, enc_min_focus, enc_avg_focus) in enumerate(sametext_batch):\n",
    "                            # factors that make up score\n",
    "                            weighted_score =  avg_prob.item() # general alignment quality\n",
    "                            weighted_score -= (max(diagonality.item(),1.11)-1.11) * diagonality_weighting  # consistent pace\n",
    "                            weighted_score -= max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting # getting stuck on pauses/phones\n",
    "                            weighted_score -= max(0.9-enc_min_focus.item(),0) * min_focus_weighting # skipping single enc outputs\n",
    "                            weighted_score -= max(2.5-enc_avg_focus.item(), 0) * avg_focus_weighting # skipping most enc outputs\n",
    "                            score_str = f\"{round(diagonality.item(),3)} {round(avg_prob.item()*100,2)}% {round(weighted_score,4)} {round(max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting,2)} {round(max(0.9-enc_min_focus.item(),0),2)}|\"\n",
    "                            if weighted_score > best_score[j]:\n",
    "                                best_score[j] = weighted_score\n",
    "                                best_score_str[j] = score_str\n",
    "                                best_generations[j] = [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "                            if show_all_attempt_scores:\n",
    "                                print(score_str, end=\"\")\n",
    "                            tries[j]+=1\n",
    "                            if np.amin(tries) >= max_attempts and np.amin(best_score) > (absolutely_required_score-1):\n",
    "                                raise StopIteration\n",
    "                            if np.amin(tries) >= absolute_maximum_tries:\n",
    "                                print(f\"Absolutely required score not achieved in {absolute_maximum_tries} attempts - \", end='')\n",
    "                                raise StopIteration\n",
    "                    \n",
    "                    if np.amin(tries) < (max_attempts-1):\n",
    "                        print('Acceptable alignment/diagonality not reached. Retrying.')\n",
    "                    elif np.amin(best_score) < absolutely_required_score:\n",
    "                        print('Score less than absolutely required score. Retrying extra.')\n",
    "            except StopIteration:\n",
    "                del batch\n",
    "                if status_updates: print(\"Done\")\n",
    "                pass\n",
    "            # [[mel, melpost, gate, align], [mel, melpost, gate, align], [mel, melpost, gate, align]] -> [[mel, mel, mel], [melpost, melpost, melpost], [gate, gate, gate], [align, align, align]]\n",
    "            \n",
    "            # zip is being weird so alternative used\n",
    "            mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = [x[0] for x in best_generations], [x[1] for x in best_generations], [x[2] for x in best_generations], [x[3] for x in best_generations] # pickup whatever was the best attempts\n",
    "            \n",
    "            # stack arrays into tensors\n",
    "            gate_batch_outputs = torch.cat(gate_batch_outputs, dim=0)\n",
    "            max_length = torch.max(gate_batch_outputs.argmax(dim=1)) # get highest duration\n",
    "            mel_batch_outputs = torch.cat(mel_batch_outputs, dim=0)[:,:,:max_length]\n",
    "            mel_batch_outputs_postnet = torch.cat(mel_batch_outputs_postnet, dim=0)[:,:,:max_length]\n",
    "            alignments_batch = torch.cat(alignments_batch, dim=0)[:,:max_length,:]\n",
    "            ##print(\"max_length =\", max_length)\n",
    "            ##print(\"gate_batch_outputs.argmax(dim=1) =\", gate_batch_outputs.argmax(dim=1))\n",
    "            ##print(\"mel_batch_outputs.shape\", mel_batch_outputs.shape)\n",
    "            ##print(\"mel_batch_outputs_postnet.shape\", mel_batch_outputs_postnet.shape)\n",
    "            ##print(\"alignments_batch.shape\", alignments_batch.shape)\n",
    "            \n",
    "            if status_updates: print(\"Running WaveGlow... \", end='')\n",
    "            audio_batch = waveglow.infer(mel_batch_outputs_postnet, speaker_ids=waveglow_speaker_ids, sigma=sigma)\n",
    "            audio_denoised_batch = denoiser(audio_batch, strength=0.0001)[:, 0]\n",
    "            if status_updates: print('Done')\n",
    "            \n",
    "            audio_len = 0\n",
    "            for j, (audio, audio_denoised) in enumerate(zip(audio_batch.split(1, dim=0), audio_denoised_batch.split(1, dim=0))):\n",
    "                # remove WaveGlow padding\n",
    "                audio_end = (gate_batch_outputs[j].argmax()+gate_delay) * hparams.hop_length\n",
    "                audio = audio[:,:audio_end]\n",
    "                audio_denoised = audio_denoised[:,:audio_end]\n",
    "                \n",
    "                # remove Tacotron2 padding\n",
    "                spec_end = gate_batch_outputs[j].argmax()+gate_delay\n",
    "                mel_outputs = mel_batch_outputs.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"mel_outputs.split(blah)[j].shape\", mel_outputs.shape)\n",
    "                mel_outputs_postnet = mel_batch_outputs_postnet.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"alignments_batch.shape\", alignments_batch.shape)\n",
    "                alignments = alignments_batch.split(1, dim=0)[j][:,:spec_end,:text_lengths[j]]\n",
    "                ##print(\"alignments.split(blah)[j].shape\", alignments.shape)\n",
    "                \n",
    "                if show_best_score:\n",
    "                    print(f\"Score: {round(best_score[j],3)}\\t\\tStats: {best_score_str[j]}  Verified: {[x.item() for x in alignment_metric(alignments)]}\")\n",
    "                if show_graphs:\n",
    "                    plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
    "                           alignments.float().data.cpu().numpy()[0].T))\n",
    "                if show_audio:\n",
    "                        ipd.display(ipd.Audio(audio.cpu().numpy(), rate=hparams.sampling_rate))        \n",
    "                if save_wavs:\n",
    "                    save_audio_path = os.path.join(audio_save_path,f\"audio_{counter//300:02}_{counter:05}.wav\")\n",
    "                    if os.path.exists(save_audio_path):\n",
    "                        if show_audio_overwrite_warnings:\n",
    "                            print(f\"File already found at [{save_audio_path}], overwriting.\")\n",
    "                        os.remove(save_audio_path)\n",
    "                    if status_updates: print(f\"Saving clip to [{save_audio_path}]... \", end=\"\")\n",
    "                    librosa.output.write_wav(save_audio_path, np.swapaxes(audio.float().cpu().numpy(),0,1), hparams.sampling_rate)\n",
    "                    if status_updates: print(\"Done\")\n",
    "                counter+=1\n",
    "                audio_len+=audio_end\n",
    "            \n",
    "            if time_to_gen:\n",
    "                audio_seconds_generated = round(audio_len.item()/hparams.sampling_rate,3)\n",
    "                print(f\"Took {round(time.time()-start_time,3)}s to generate {audio_seconds_generated}s of audio. (best of {tries.sum().astype('int')} tries)\")\n",
    "                #print(\"spec_end/max_len = \", spec_end/max_len) # debug\n",
    "                #print(\"spec_end/max_len = \", spec_end/max_len) # debug\n",
    "            \n",
    "            print(\"\")\n",
    "_text = None; _audio_path = None; _speaker_id = None\n",
    "\n",
    "# Merge clips and output the concatenated result\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# get number of intermediate concatenations required (Sox and only merge 340~ files at a time)\n",
    "n_audio_batches = round(len( glob(os.path.join(audio_save_path, \"audio_*_*.wav\")) ) / 300)\n",
    "\n",
    "# ensure path ends in .wav\n",
    "if not output_filename[-4:].lower() == '.wav':\n",
    "    output_filename+='.wav'\n",
    "\n",
    "for i in range(n_audio_batches):\n",
    "    print(f\"Merging audio files {i*300} to {((i+1)*300)-1}... \", end='')\n",
    "    os.system(f'sox {os.path.join(audio_save_path, f\"audio_{i:02}_*.wav\")} -b 16 {os.path.join(audio_save_path, f\"concat_{i:02}.wav\")}')\n",
    "    print(\"Done\")\n",
    "print(f\"Saving output to '{os.path.join(audio_save_path, output_filename)}'... \", end='')\n",
    "os.system(f'sox \"{os.path.join(audio_save_path, \"concat_*.wav\")}\" -b 16 \"{os.path.join(audio_save_path, output_filename)}\"') # merge the merged files into a final output. bit depth of 16 required to go over 4 hour length\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(64.5/1024)/1.29\n",
    "# 0.048828125 MB per second of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.048828125 * (74*3600))*(600//100) / 1024\n",
    "# approx 3811 GB to store all the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice( list(range(0,600,100)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"filename.mel100.npy\".split(\".mel\")[1].split(\".npy\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(r\"D:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S1\\s1e1\\00_00_05_Celestia_Neutral__Once upon a time.npy\").dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = (np.ones(5) * int(2**31)).astype('int32')\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x.astype(np.float32)/int(2147483648))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "x = [0,1,2,3]\n",
    "y = ['A','B','C','D']\n",
    "z = list(zip(x,y))\n",
    "\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(type(y))\n",
    "print(y)\n",
    "print(y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "x = [0,1,2,3]\n",
    "y = ['A','B','C','D']\n",
    "z = list(zip(x,y))\n",
    "\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(type(y))\n",
    "print(y)\n",
    "print(y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in zip(torch.rand(5), torch.zeros(5)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([0,1],[1,2]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0]*3\n",
    "x[0] = \"blah\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str text file\n",
    "# arr split by quotes\n",
    "# arr split by periods and quotes based on length\n",
    "# generator[arr] split by text batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show saved Postnet Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "#filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2\"\n",
    "files = sorted(glob(filepath+\"/**/*.mel.npy\", recursive=True))\n",
    "start = int(random.random() * len(files))\n",
    "file_count = 10\n",
    "for i in range(start, start+file_count):\n",
    "    #file = random.choice(files)\n",
    "    file = files[i]\n",
    "    H = np.load(file)\n",
    "    H = (H+5.2)*0.5\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(file+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "    # again\n",
    "    file = file.replace(\".mel.npy\",\".npy\")\n",
    "    H = np.load(file)\n",
    "    H = (H+5.2)*0.5\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(file+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of GT Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "#filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/FiM/S1/s1e1\"\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2\"\n",
    "files = sorted(glob(filepath+\"/**/*.npy\", recursive=True))\n",
    "total_size = 0\n",
    "min_duration = 0.6\n",
    "SR = 48000\n",
    "BD = 2 # 16 bits\n",
    "for path in files:\n",
    "    file_size = os.stat(path).st_size\n",
    "    if file_size > (SR*BD*min_duration):\n",
    "        total_size+=file_size\n",
    "\n",
    "duration = total_size / (SR*BD)\n",
    "duration_min = duration/60\n",
    "duration_hrs = duration/3600\n",
    "total_size_MB = total_size / (1024**3)\n",
    "print(f\"{total_size_MB} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size & Duration of Wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "def get_stats(filepath, ext='.wav'):\n",
    "    import soundfile as sf\n",
    "    files = sorted(glob(filepath+f\"/**/*{ext}\", recursive=True))\n",
    "    total_size = 0\n",
    "    total_duration = 0\n",
    "    min_duration = 0.6\n",
    "    SR = 48000\n",
    "    BD = 2 # 16 bits\n",
    "    for path in files:\n",
    "        file_size = os.stat(path).st_size\n",
    "        audio, samplerate = sf.read(path)\n",
    "        if len(audio)/samplerate > min_duration:\n",
    "            total_size+=file_size\n",
    "            total_duration+=len(audio)/samplerate\n",
    "    duration_min = total_duration/60\n",
    "    duration_hrs = total_duration/3600\n",
    "    print(f\"{total_duration} seconds = {duration_min} minutes = {duration_hrs} hours\")\n",
    "    total_size_MB = total_size / (1024**3)\n",
    "    print(f\"{total_size_MB} GB of wavs\")\n",
    "\n",
    "get_stats(r\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/Blizzard2011\", ext='.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing FP16 vs FP32 individual Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().half()\n",
    "#########################################################\n",
    "def b_inv(b_mat):\n",
    "    eye = b_mat.new_ones(b_mat.size(-1)).diag().expand_as(b_mat).float()\n",
    "    b_inv, _ = torch.solve(eye.float(), b_mat.float())\n",
    "    return b_inv\n",
    "fp16 = b_inv(weight.squeeze())\n",
    "#########################################################\n",
    "print(fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().float()\n",
    "#########################################################\n",
    "fp32 = weight.squeeze().inverse()\n",
    "#########################################################\n",
    "print(fp32)\n",
    "#assert torch.equal(fp32.float(), fp16.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.slogdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().slogdet()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().logdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().det().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10000\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(500,1,1).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().log().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10000\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(500,1,1).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().half().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().half().log()\n",
    "print(log_det_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda()\n",
    "weight.half()\n",
    "weight.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Multiple WaveGlow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from denoiser import Denoiser\n",
    "from glob import glob\n",
    "from random import random\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "from layers import TacotronSTFT, STFT\n",
    "from utils import load_wav_to_torch\n",
    "from hparams import create_hparams\n",
    "%matplotlib inline\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "def plot_audio_spec(audio, sampling_rate=48000):\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def load_model(waveglow_path):\n",
    "    from efficient_model import WaveGlow\n",
    "    from efficient_util import remove_weight_norms\n",
    "    import json\n",
    "    data = r\"\"\"{\n",
    "        \"train_config\": {\n",
    "            \"fp16_run\": false,\n",
    "            \"output_directory\": \"outdir_EfficientBaseline\",\n",
    "            \"epochs\": 1000,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"sigma\": 1.0,\n",
    "            \"iters_per_checkpoint\": 2000,\n",
    "            \"batch_size\": 30,\n",
    "            \"seed\": 1234,\n",
    "            \"checkpoint_path\": \"outdir_EfficientBaseline/waveglow_1425\",\n",
    "            \"with_tensorboard\": true\n",
    "        },\n",
    "        \"data_config\": {\n",
    "            \"training_files\": \"map_0_GT.txt\",\n",
    "            \"segment_length\": 19200,\n",
    "            \"sampling_rate\": 48000,\n",
    "            \"filter_length\": 2400,\n",
    "            \"hop_length\": 600,\n",
    "            \"win_length\": 2400,\n",
    "            \"mel_fmin\": 0.0,\n",
    "            \"mel_fmax\": 16000.0\n",
    "        },\n",
    "        \"dist_config\": {\n",
    "            \"dist_backend\": \"nccl\",\n",
    "            \"dist_url\": \"tcp://127.0.0.1:54321\"\n",
    "        },\n",
    "        \"waveglow_config\": {\n",
    "            \"n_mel_channels\": 160,\n",
    "            \"n_flows\": 12,\n",
    "            \"n_group\": 8,\n",
    "            \"n_early_every\": 4,\n",
    "            \"n_early_size\": 2,\n",
    "            \"memory_efficient\": false,\n",
    "            \"WN_config\": {\n",
    "                \"dilation_channels\":256,\n",
    "                \"residual_channels\":256,\n",
    "                \"skip_channels\":256,\n",
    "                \"n_layers\": 9,\n",
    "                \"radix\": 3,\n",
    "                \"bias\": true\n",
    "            }\n",
    "        }\n",
    "    }\"\"\"\n",
    "    config = json.loads(data)\n",
    "    train_config = config[\"train_config\"]\n",
    "    global data_config\n",
    "    data_config = config[\"data_config\"]\n",
    "    global dist_config\n",
    "    dist_config = config[\"dist_config\"]\n",
    "    global waveglow_config\n",
    "    waveglow_config = { \n",
    "        **config[\"waveglow_config\"], \n",
    "        'win_length': data_config['win_length'],\n",
    "        'hop_length': data_config['hop_length']\n",
    "    }\n",
    "    waveglow = WaveGlow(**waveglow_config).cuda()\n",
    "    waveglow_dict = torch.load(waveglow_path)['model'].state_dict()\n",
    "    waveglow.load_state_dict(waveglow_dict)\n",
    "    waveglow.apply(remove_weight_norms)\n",
    "    waveglow.cuda().eval()#.half()\n",
    "    #for k in waveglow.convinv:\n",
    "    #    k.float()\n",
    "    #denoiser = Denoiser(waveglow)\n",
    "    waveglow_iters = torch.load(waveglow_path)['iteration']\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    return waveglow, waveglow_iters\n",
    "\n",
    "\n",
    "def waveglow_infer(mel_outputs_postnet, sigma_, iters=''):\n",
    "    current_audio = waveglow.infer(mel_outputs_postnet, sigma=sigma_).unsqueeze(0)\n",
    "    audio.append(current_audio)\n",
    "    print(\"sigma = {}\".format(sigma_)); ipd.display(ipd.Audio(audio[len(audio)-1][0].data.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "    maxv = np.iinfo(np.int16).max\n",
    "    sf.write(\"infer/temp.wav\", (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path+f\"\\nAfter WaveGlow {iters}\\nSigma {sigma_}\")\n",
    "\n",
    "\n",
    "def waveglow_infer_filepath(file_path, sigma_, iters=''):\n",
    "    mel_outputs_postnet = np.load(file_path)\n",
    "    mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4\n",
    "    mel_outputs_postnet = torch.from_numpy(mel_outputs_postnet).unsqueeze(0).cuda()#.half()\n",
    "    audio = []\n",
    "    with torch.no_grad():\n",
    "        waveglow_infer(mel_outputs_postnet, sigma_, iters=iters)\n",
    "\n",
    "audio = []\n",
    "hparams = create_hparams()\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*.npy\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"outdir_EfficientBaseline/waveglow_5158\n",
    "outdir_EfficientBaseline/waveglow_5483\n",
    "outdir_EfficientBaseline/waveglow_5666\n",
    "outdir_EfficientBaseline/waveglow_7264\n",
    "outdir_EfficientBias/waveglow_252\n",
    "outdir_EfficientBias/waveglow_655\n",
    "outdir_EfficientBias/waveglow_1003\n",
    "outdir_EfficientBias/waveglow_1935\n",
    "outdir_EfficientBias/waveglow_11346\n",
    "outdir_EfficientBias/waveglow_11863\n",
    "outdir_EfficientBias/waveglow_12118\n",
    "outdir_EfficientBias/waveglow_12282\n",
    "outdir_EfficientBias/waveglow_14197\n",
    "outdir_EfficientBias/waveglow_16058\"\"\"\n",
    "model_paths = r\"\"\"\n",
    "outdir_EfficientBias/waveglow_365\n",
    "\"\"\".split(\"\\n\")\n",
    "\n",
    "disp_mel(load_mel(file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")).squeeze(), desc=file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")+f\"\\nGround Truth\")\n",
    "for model_path in [path for path in model_paths if path]:\n",
    "    waveglow, iters = load_model(f\"/media/cookie/Samsung 860 QVO/TTCheckpoints/waveglow/{model_path}\")\n",
    "    waveglow_infer_filepath(file_path, 0.9, iters=iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveGlow GTA Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from denoiser import Denoiser\n",
    "from glob import glob\n",
    "from random import random\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "from layers import TacotronSTFT, STFT\n",
    "from utils import load_wav_to_torch\n",
    "from hparams import create_hparams\n",
    "from shutil import copyfile\n",
    "\n",
    "from waveglow_utils import PreEmphasis, InversePreEmphasis\n",
    "%matplotlib inline\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\nLength: \"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "def plot_audio_spec(audio, sampling_rate=48000):\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "hparams = create_hparams()\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "#stft_wideband = TacotronSTFT(hparams.filter_length, hparams.hop_length, 1024,\n",
    "#                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "#                    hparams.mel_fmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveglow(waveglow_path):\n",
    "    waveglow_dict = torch.load(waveglow_path)\n",
    "    waveglow = waveglow_dict['model']\n",
    "    waveglow_iters = int(waveglow_dict['iteration'])\n",
    "    waveglow.cuda().eval().half()\n",
    "    denoiser = Denoiser(waveglow)\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    if not hasattr(waveglow, \"spect_scaling\"):\n",
    "        setattr(waveglow, \"spect_scaling\", False)\n",
    "    return waveglow, denoiser, waveglow_iters\n",
    "\n",
    "def load_waveglow_yoyo(waveglow_path):\n",
    "    waveglow_dict = torch.load(waveglow_path)\n",
    "    waveglow = waveglow_dict['model']\n",
    "    waveglow_iters = int(waveglow_dict['iteration'])\n",
    "    waveglow.cuda().eval()#.half()\n",
    "    denoiser = None#Denoiser(waveglow)\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    return waveglow, denoiser, waveglow_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waveglow_dict = torch.load(\"/media/cookie/Samsung 860 QVO/TTCheckpoints/waveglow/outdir_EfficientLarge/best_model\")\n",
    "waveglow = waveglow_dict['model']\n",
    "waveglow_iters = int(waveglow_dict['iteration'])\n",
    "waveglow.cuda().eval()#.half()\n",
    "denoiser = None#Denoiser(waveglow)\n",
    "print(waveglow_iters, \"iterations\")\n",
    "print(waveglow.WNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(waveglow.WNs[0].WN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Random File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveglow_infer(mel_outputs_postnet, sigma_, deempathsis, root_dir='infer', clip_folder='', filename=''):\n",
    "    current_audio = waveglow.infer(mel_outputs_postnet, sigma=sigma_)\n",
    "    if len(current_audio.shape) == 1:\n",
    "        current_audio = current_audio.unsqueeze(0)\n",
    "    if deempathsis:\n",
    "        deempthasis_filter = InversePreEmphasis(float(deempathsis)) # TODO, replace with something lightweight.\n",
    "        current_audio = deempthasis_filter(current_audio.cpu().float().unsqueeze(0)).squeeze(0).cuda()\n",
    "    audio.append(current_audio)\n",
    "    \n",
    "    # Show Audio for Listening in Notebook\n",
    "    #ipd.display(ipd.Audio(audio[len(audio)-1][0].data.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "    \n",
    "    # Save Audio\n",
    "    local_fpath = os.path.join(root_dir, clip_folder, filename) # local filepath\n",
    "    local_dpath = os.path.join(root_dir, clip_folder) # local directory path\n",
    "    os.makedirs(local_dpath, exist_ok=True) # ensure local directory exists\n",
    "    maxv = np.iinfo(np.int16).max # get max int16 value\n",
    "    sf.write(os.path.join(root_dir, \"temp.wav\"), (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate) # write audio to temp\n",
    "    \n",
    "    # Get MSE and MAE\n",
    "    waveglow_spect = load_mel(os.path.join(root_dir, \"temp.wav\")).squeeze() # load spectrogram from wav file.\n",
    "    waveglow_spect_lossy = (waveglow_spect.unsqueeze(0).cuda().half()[:,:,:mel_outputs_postnet.shape[-1]])#+5.2)*0.5 # move spectrogram to GPU, reshape and normalize within -4, 4.\n",
    "    MSE = (nn.MSELoss()(waveglow_spect_lossy, mel_outputs_postnet)).item() # get MSE (Mean Squared Error) between Ground Truth and WaveGlow inferred spectrograms.\n",
    "    MAE = (nn.L1Loss()(waveglow_spect_lossy, mel_outputs_postnet)).item() # get MAE (Mean Absolute Error) between Ground Truth and WaveGlow inferred spectrograms.\n",
    "    \n",
    "    #sf.write(local_fpath+f\"-MSE_{round(MSE,4)}.wav\", (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate) # write audio to fpath\n",
    "    sf.write(local_fpath+f\"-MSE_{round(MSE,4)}.wav\", np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1), hparams.sampling_rate, \"PCM_16\") # write audio to fpath\n",
    "    # Show Spect\n",
    "    #disp_mel(waveglow_spect, desc=f\"\\nAfter WaveGlow\\nSigma: {sigma_}\\nMSELoss: {MSE}\") # Show Plot in Notebook\n",
    "    return MSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*__*.npy\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveglow_infer_paths = [\n",
    "    \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/Special source/s6e24/00_04_52_Rainbow_Neutral__That's a mighty big claim considering everypony here is an amazingly awesome crazy good flyer.mel.npy\"\n",
    "]\n",
    "for i in range(19):\n",
    "    file_path = files[int(random()*len(files))]\n",
    "    waveglow_infer_paths.append(file_path)\n",
    "print(\"\".join([x+\"\\n\" for x in waveglow_infer_paths]))# Print each entry in multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"\"\"\n",
    "# Baseline, 12 Flow, 256 Channel, 8 Layer, 0.00 Empthasis\n",
    "outdir_twilight9/waveglow_140900|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_152256|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_187539|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_229258|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_268276|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_311477|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_334361|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_351862|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_387823|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_422080|1|0|0.00|0\n",
    "# Mini (ReZero), 12 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_30000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_38265|1|0|0.97|0\n",
    "# Mini, 12 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_2598|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_3979|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_6496|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_13894|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_15659|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_35439|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_40000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_55133|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_100000|1|0|0.9|07\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_120000|1|0|0.9|07\n",
    "# Mini, 16 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_11306|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_55528|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_57259|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_69333|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_77184|1|0|0.97|0\n",
    "# Mini, 24 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_66559|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_70000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_80000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_96555|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_108044|1|0|0.97|0\n",
    "# LARGE, 16 Flow, 512 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_EfficientLarge/waveglow_60000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_70000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_80000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_90000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_120119|1|0|0.97|1\n",
    "outdir_EfficientLarge/best_model|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_127217|1|0|0.97|1\n",
    "# Baseline (Nancy Datset Only), 12 Flow, 256 Channel, 8 Layer, 0.00 Empthasis\n",
    "\"\"\"\n",
    "\n",
    "# path|normalize(-4 to 4)|mu_law_quantization|de-empthasis|yoyololicon version\n",
    "waveglow_paths = \"\"\"\n",
    "outdir_twilight9/waveglow_140900|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_152256|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_187539|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_229258|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_268276|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_311477|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_334361|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_351862|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_387823|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_422080|1|0|0.00|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_30000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_38265|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_2598|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_3979|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_6496|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_13894|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_15659|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_35439|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_40000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_55133|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_100000|1|0|0.9|07\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_120000|1|0|0.9|07\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_11306|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_55528|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_57259|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_69333|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_77184|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_66559|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_70000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_80000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_96555|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_108044|1|0|0.97|0\n",
    "outdir_EfficientLarge/waveglow_60000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_70000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_80000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_90000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_120119|1|0|0.97|1\n",
    "outdir_EfficientLarge/best_model|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_127217|1|0|0.97|1\n",
    "outdir_NancyOnly/best_model|0|0|0.00|0\n",
    "\"\"\"[1:-1].split(\"\\n\")\n",
    "\n",
    "print(\"Missing Checkpoints:\")\n",
    "print(\"\\n\".join([x.split(\"|\")[0] for x in waveglow_paths if not os.path.exists(\"../tacotron2/waveglow_latest/\"+x.split(\"|\")[0])]))\n",
    "waveglow_paths = [x for x in waveglow_paths if os.path.exists(\"../tacotron2/waveglow_latest/\"+x.split(\"|\")[0])]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for waveglow_meta in waveglow_paths:\n",
    "        print(\"--------------------------------------------------------\")\n",
    "        waveglow_path, normalize_spec, mu_law_quantization, deempthasis_strength, b_yoyololicon_model = waveglow_meta.split(\"|\")\n",
    "        waveglow_info = str(\"_\".join(waveglow_path.split(\"/\")[0].split(\"_\")[1:]))\n",
    "        print(waveglow_meta)\n",
    "        if b_yoyololicon_model:\n",
    "            waveglow, denoiser, waveglow_current_iter = load_waveglow_yoyo(\"../tacotron2/waveglow_latest/\"+waveglow_path)\n",
    "        else:\n",
    "            waveglow, denoiser, waveglow_current_iter = load_waveglow(\"../tacotron2/waveglow_latest/\"+waveglow_path)\n",
    "        \n",
    "        if not hasattr(waveglow, \"spect_scaling\"):\n",
    "            setattr(waveglow, \"spect_scaling\", False)\n",
    "        \n",
    "        audio = []\n",
    "        total_MAE = total_MSE = 0\n",
    "        best_MAE = best_MSE = 9e9\n",
    "        worst_MAE = worst_MSE = -9e9\n",
    "        for file_path in waveglow_infer_paths:\n",
    "            #print(f\"FILE: {file_path}\") # Print the file path\n",
    "            basename = os.path.splitext(os.path.basename(file_path.replace(\".mel.npy\",\".npy\")))[0] # filename without ext\n",
    "            \n",
    "            mel_outputs_postnet = np.load(file_path) # Load Tacotron2 Postnet Outputs\n",
    "            if int(normalize_spec):\n",
    "                mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4\n",
    "            \n",
    "            #disp_mel(load_mel(file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")).squeeze(), desc=f\"\\nGround Truth\") # Display Ground Truth Spectrogram\n",
    "            #disp_mel(mel_outputs_postnet, desc=\"\\nThis is the original Postnet output from Tacotron\") # Display Tacotron GTA Spectrogram\n",
    "            \n",
    "            mel_outputs_postnet = np.load(file_path.replace(\".mel.npy\",\".npy\")) # Load Ground Truth Spectrogram for inference by WaveGlow.\n",
    "            if int(normalize_spec):\n",
    "                mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4, speeds up initial training\n",
    "            mel_outputs_postnet = torch.from_numpy(mel_outputs_postnet).unsqueeze(0).cuda() # prep tensor for WaveGlow.\n",
    "            if not b_yoyololicon_model:\n",
    "                mel_outputs_postnet = mel_outputs_postnet.half()\n",
    "            \n",
    "            sigma = 0.9\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            sigma = 0.95\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            sigma = 1.0\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            \n",
    "            if not os.path.exists(os.path.join(\"infer\", basename, f'GroundTruth.wav')):\n",
    "                copyfile(os.path.splitext(file_path.replace(\".mel.npy\",\".npy\"))[0]+\".wav\", os.path.join(\"infer\", basename, f'GroundTruth.wav'))\n",
    "            total_MSE+=MSE\n",
    "            best_MSE = min(best_MSE, MSE)\n",
    "            worst_MSE = max(worst_MSE, MSE)\n",
    "            total_MAE+=MAE\n",
    "            best_MAE = min(best_MAE, MAE)\n",
    "            worst_MAE = max(worst_MAE, MAE)\n",
    "        print(f\"Average MSE: {total_MSE/len(waveglow_infer_paths)} Best MSE: {best_MSE} Worst MSE: {worst_MSE}\")\n",
    "        print(f\"Average MAE: {total_MAE/len(waveglow_infer_paths)} Best MAE: {best_MAE} Worst MAE: {worst_MAE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = torch.rand(4096,1024,2,1, device=\"cuda:0\")\n",
    "x = x + 10\n",
    "x = x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = torch.rand(4096,1024,2,1, device=\"cuda:0\")\n",
    "x = x + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(waveglow, \"spect_scaling\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(waveglow.parameters())[0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 2\n",
    "n_group = 8\n",
    "audio = torch.arange(32).repeat(batch_dim, 1)\n",
    "print(audio)\n",
    "audio = audio.view(batch_dim, -1, n_group).transpose(1, 2)\n",
    "print(\"shape =\", audio.shape)\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,6,1)\n",
    "print(x)\n",
    "y_0, y_1 = x.chunk(2,1)\n",
    "print(\"y_0 =\\n\", y_0)\n",
    "print(\"y_1 =\\n\", y_1)\n",
    "\n",
    "n_half = int(x.size(1)/2)\n",
    "y_0 = x[:,:n_half,:]\n",
    "y_1 = x[:,n_half:,:]\n",
    "print(\"y_0 =\\n\", y_0)\n",
    "print(\"y_1 =\\n\", y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)\n",
    "print(x)\n",
    "x.mul_(2).add_(-5)\n",
    "print(x.view(-1))\n",
    "x = x.unsqueeze(0)\n",
    "print(x.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (torch.rand(5)-0.5)*2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x*x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.abs(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\n",
    "    torch.cuda.get_device_properties(\"cuda:0\").total_memory/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_allocated(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.max_memory_allocated(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_reserved(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.max_memory_reserved(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_summary(\"cuda:0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoise_strength = 10\n",
    "for i in audio:\n",
    "    audio_denoised = denoiser(i, strength=denoise_strength)[:, 0]\n",
    "    ipd.display(ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "maxv = np.iinfo(np.int16).max\n",
    "sf.write(\"infer/temp.wav\", (np.swapaxes(audio_denoised.cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate)\n",
    "disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path+f\"\\nAfter Denoise\\nDenoise Strength {denoise_strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,5)\n",
    "print(x)\n",
    "y = x*torch.tensor([0,1,0,1,0])\n",
    "print(y)\n",
    "# masked_fill_ should be a little more performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "truncated_length = 100\n",
    "lengths = torch.tensor([268, 239, 296, 148, 87, 453, 601, 602, 603, 604, 605, 606, 607, 608, 609])\n",
    "processed = 0\n",
    "\n",
    "batch_lengths = lengths[:batch_size]\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"-\"*100)\n",
    "    print(batch_lengths)\n",
    "    print((batch_lengths-truncated_length))\n",
    "    print((batch_lengths-truncated_length)[batch_lengths-truncated_length>0])\n",
    "    print((batch_lengths-truncated_length)[batch_lengths-truncated_length>0].shape[0])\n",
    "    print(batch_size - (batch_lengths-truncated_length)[batch_lengths-truncated_length>0].shape[0])\n",
    "    \n",
    "    #batch_lengths = (batch_lengths-truncated_length)[batch_lengths-truncated_length>0]\n",
    "    print(batch_lengths)\n",
    "    print(\"processed =\",processed)\n",
    "    processed+=batch_size-((batch_lengths-truncated_length)[batch_lengths-truncated_length>0]).shape[0]\n",
    "    batch_lengths = torch.cat((batch_lengths, lengths[processed+batch_size:processed+batch_size+(batch_size-batch_lengths.shape[0])]), 0)\n",
    "    print(batch_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "processed = 0\n",
    "lengths = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "x = lengths[:batch_size]\n",
    "print(x)\n",
    "print(x[x<4].shape[0])\n",
    "x[x<4] = lengths[processed+batch_size:processed+batch_size+x[x<4].shape[0]]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Pre-empthasis for Audio Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from random import random\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from waveglow_utils import PreEmphasis, InversePreEmphasis\n",
    "preempthasis_strength = 0.97\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*__*.wav\", recursive=True))\n",
    "\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/VCTK-Corpus-0.92/wav\"\n",
    "files = sorted(glob(filepath+\"/**/*.wav\", recursive=True))\n",
    "\n",
    "preempth_filter = PreEmphasis(preempthasis_strength).float()\n",
    "deempth_filter = InversePreEmphasis(preempthasis_strength).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio, sample_rate = sf.read(file_path)\n",
    "    print(audio.max())\n",
    "    print(audio.min())\n",
    "    print(\"Original\")\n",
    "    ipd.display(ipd.Audio(audio, rate=sample_rate))\n",
    "    \n",
    "    sf.write(\"infer/temp_original.wav\", audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp_original.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    import scipy\n",
    "    from scipy import signal\n",
    "    sos = signal.butter(10, 60, 'hp', fs=48000, output='sos')\n",
    "    filtered_audio = signal.sosfilt(sos, audio)\n",
    "    \n",
    "    sf.write(\"infer/temp.wav\", filtered_audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    sos = signal.butter(2, 60, 'hp', fs=48000, output='sos')\n",
    "    filtered_audio = signal.sosfilt(sos, audio)\n",
    "    \n",
    "    sf.write(\"infer/temp.wav\", filtered_audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    maxv = np.iinfo(np.int16).max\n",
    "    audio = deempth_filter((torch.tensor(audio)/maxv).unsqueeze(0).unsqueeze(0).float())\n",
    "    print(audio.mean())\n",
    "    print(audio.std())\n",
    "    print(\"De-Empthasis\")\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))\n",
    "    \n",
    "    audio, sample_rate = sf.read(file_path)\n",
    "    audio = preempth_filter((torch.tensor(audio)/maxv).unsqueeze(0).unsqueeze(0).float())\n",
    "    print(\"Pre-Empthasis\")\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))\n",
    "    \n",
    "    print(\"Pre-Empthasis + De-Empthasis\")\n",
    "    audio = deempth_filter(audio)\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mcd(C, C_hat):\n",
    "    \"\"\"C and C_hat are NumPy arrays of shape (T, D),\n",
    "    representing mel-cepstral coefficients.\n",
    "\n",
    "    \"\"\"\n",
    "    K = 10 / np.log(10) * np.sqrt(2)\n",
    "    return K * np.mean(np.sqrt(np.sum((C - C_hat) ** 2, axis=1)))\n",
    "mcd(np.array([[0,0.8,0]]),np.array([[0,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force Loading Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = {\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "    'e':5,\n",
    "}\n",
    "modeldict = {\n",
    "    'c':3,\n",
    "    'e':4,\n",
    "    'f':5,\n",
    "}\n",
    "dummy_modeldict = {k: v for k,v in pretrained.items() if k in modeldict and pretrained[k] == modeldict[k]}\n",
    "model_dict_missing = {k: v for k,v in pretrained.items() if k not in modeldict}\n",
    "model_dict_mismatching = {k: v for k,v in pretrained.items() if k in modeldict and pretrained[k] != modeldict[k]}\n",
    "pretrained_missing = {k: v for k,v in modeldict.items() if k not in pretrained}\n",
    "print(list(model_dict_missing.keys()),'does not exist in the current model')\n",
    "print(list(model_dict_mismatching.keys()),\"is the wrong shape and has been reset\")\n",
    "print(list(pretrained_missing.keys()),\"doesn't have pretrained weights and is reset\")\n",
    "print(dummy_modeldict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Blur to Spectrograms during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_mel(\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/Special source/s6e24/00_03_22_Rainbow_Neutral__tell ya what. I'll leave the teaching stuff to you And I'll just make sure they stay awake.wav\")\n",
    "scale_embed = torch.rand([1,160])*0.5 + 0.55\n",
    "x = x*scale_embed.unsqueeze(2)\n",
    "print(x.shape)\n",
    "print(scale_embed.shape)\n",
    "print(scale_embed)\n",
    "x = x.cuda()\n",
    "disp_mel(x.squeeze().cpu())\n",
    "\n",
    "x_strength = 100.0\n",
    "y_strength = 0.001\n",
    "filter_cycles = 3\n",
    "filter = kornia.filters.GaussianBlur2d((3,3),(x_strength,y_strength))\n",
    "\n",
    "#print(x.shape)\n",
    "# filter input needs (B,C,H,W)\n",
    "out = filter(x.unsqueeze(0))\n",
    "for i in range(filter_cycles-1): out = filter(out)\n",
    "disp_mel(out.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask for Tacotron Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, out=torch.LongTensor(max_len))\n",
    "    mask = (ids < lengths.unsqueeze(1))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,8)\n",
    "output_lengths = torch.tensor([8,5,2,1])\n",
    "print(x)\n",
    "print(output_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ~get_mask_from_lengths(output_lengths)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.expand_as(x)\n",
    "print(y)\n",
    "x.masked_fill_(y, 1e3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
