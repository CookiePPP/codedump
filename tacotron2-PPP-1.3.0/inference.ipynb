{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Start) Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (End) Testing Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tacotron 2 inference code \n",
    "Edit the variables **checkpoint_path** and **text** to match yours and run the entire code to generate plots of mel outputs, alignments and audio synthesis from the generated mel-spectrogram using Griffin-Lim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and setup matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Tacotron2\n",
    "from layers import TacotronSTFT, STFT\n",
    "from audio_processing import griffin_lim\n",
    "from train import load_model\n",
    "from text import text_to_sequence\n",
    "from denoiser import Denoiser\n",
    "from unidecode import unidecode\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, info=None):\n",
    "    %matplotlib inline\n",
    "    fig, axes = plt.subplots(len(data), 1, figsize=(int(alignment_graph_width*graph_scale/100), int(alignment_graph_height*graph_scale/100)))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(len(data)):\n",
    "        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
    "                       interpolation='none', cmap='inferno')\n",
    "    axes[0].set(xlabel=\"Frames\", ylabel=\"Channels\")\n",
    "    axes[1].set(xlabel=\"Decoder timestep\", ylabel=\"Encoder timestep\")\n",
    "    fig.canvas.draw()\n",
    "    plt.show()\n",
    "\n",
    "#dictionary_path = r\"/media/cookie/Samsung PM961/TwiBot/tacotron2/filelists/merged.dict_.txt\"\n",
    "dictionary_path = r\"G:\\TwiBot\\tacotron2\\filelists\\merged.dict_.txt\"\n",
    "print(\"Running, Please wait...\")\n",
    "thisdict = {}\n",
    "for line in reversed((open(dictionary_path, \"r\").read()).splitlines()):\n",
    "    thisdict[(line.split(\" \", 1))[0]] = (line.split(\" \", 1))[1].strip()\n",
    "print(\"Dictionary Ready.\")\n",
    "sym = list(\"☺☻♥♦♣♠•◘○◙♂♀♪♫☼►◄↕‼¶§▬↨↑↓→←∟↔▲▼\")\n",
    "def ARPA(text_, punc=r\"!?,.;:␤#-_'\\\"()[]\"):\n",
    "    text = text_.replace(\"\\n\",\" \"); out = ''\n",
    "    for word_ in text.split(\" \"):\n",
    "        word=word_; end_chars = ''; start_chars = ''\n",
    "        while any(elem in word for elem in punc) and len(word) > 1:\n",
    "            if word[-1] in punc: end_chars = word[-1] + end_chars; word = word[:-1]\n",
    "            elif word[0] in punc: start_chars = start_chars + word[0]; word = word[1:]\n",
    "            else: break\n",
    "        try: word_arpa = thisdict[word.upper()]\n",
    "        except: word_arpa = ''\n",
    "        if len(word_arpa)!=0: word = \"{\" + str(word_arpa) + \"}\"\n",
    "        out = (out + \" \" + start_chars + word + end_chars).strip()\n",
    "    #if out[-1] != \"␤\": out = out + \"␤\"\n",
    "    #if out[0] != \"☺\": out = \"☺\" + out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = create_hparams()\n",
    "hparams.max_decoder_steps = 1000\n",
    "hparams.gate_threshold = 0.6\n",
    "hparams.ignore_layers = []\n",
    "print(str(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows WaveGlow from Ground Truth\n",
    "from utils import load_wav_to_torch\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Tacotron2 model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_41000\"\n",
    "print(\"Loading Tacotron... \", end=\"\")\n",
    "checkpoint_hparams = torch.load(checkpoint_path)['hparams']\n",
    "#checkpoint_hparams.parse_json(hparams.to_json())\n",
    "model = load_model(hparams)\n",
    "#checkpoint_dict = {k.replace(\"encoder_speaker_embedding.weight\",\"encoder.encoder_speaker_embedding.weight\"): v for k,v in torch.load(checkpoint_path)['state_dict'].items()}\n",
    "checkpoint_dict = torch.load(checkpoint_path)['state_dict']\n",
    "model.load_state_dict(checkpoint_dict)\n",
    "_ = model.cuda().eval().half(); print(\"Done\")\n",
    "\n",
    "tacotron_speaker_id_lookup = torch.load(checkpoint_path)['speaker_id_lookup']\n",
    "print(\"This Tacotron model has been trained for \",torch.load(checkpoint_path)['iteration'],\" Iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load WaveGlow from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "waveglow_path = r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing2\\best_val_model\"\n",
    "config_fpath = r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing2\\config.json\"\n",
    "\n",
    "# Load config file\n",
    "with open(config_fpath) as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "train_config = config[\"train_config\"]\n",
    "global data_config\n",
    "data_config = config[\"data_config\"]\n",
    "global dist_config\n",
    "dist_config = config[\"dist_config\"]\n",
    "global waveglow_config\n",
    "waveglow_config = {\n",
    "    **config[\"waveglow_config\"], \n",
    "    'win_length': data_config['win_length'],\n",
    "    'hop_length': data_config['hop_length']\n",
    "}\n",
    "print(waveglow_config)\n",
    "print(f\"Config File from '{config_fpath}' successfully loaded.\")\n",
    "\n",
    "# import the correct model\n",
    "if waveglow_config[\"yoyo\"]: # efficient_mode # TODO: Add to Config File\n",
    "    from efficient_model import WaveGlow\n",
    "else:\n",
    "    from glow import WaveGlow\n",
    "\n",
    "# initialize model\n",
    "print(f\"intializing WaveGlow model... \", end=\"\")\n",
    "waveglow = WaveGlow(**waveglow_config).cuda()\n",
    "print(f\"Done!\")\n",
    "\n",
    "# load checkpoint from file\n",
    "print(f\"loading WaveGlow checkpoint... \", end=\"\")\n",
    "checkpoint = torch.load(waveglow_path)\n",
    "waveglow.load_state_dict(checkpoint['model']) # and overwrite initialized weights with checkpointed weights\n",
    "waveglow.cuda().eval().half() # move to GPU and convert to half precision\n",
    "print(f\"Done!\")\n",
    "#for k in waveglow.convinv:\n",
    "#    k.float()\n",
    "print(f\"initializing Denoiser... \", end=\"\")\n",
    "denoiser = Denoiser(waveglow)\n",
    "print(f\"Done!\")\n",
    "waveglow_iters = torch.load(waveglow_path)['iteration']\n",
    "print(waveglow_iters, \"iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 1) Get Speaker ID's from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tacotron_speaker_id_lookup\n",
    "waveglow_speaker_id_lookup = checkpoint['speaker_lookup']\n",
    "print(str(waveglow_speaker_id_lookup).replace(\",\",\"\\n\").replace(\":\",\" ->\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 2) Rebuild Speaker ID's from training filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_utils import TextMelLoader\n",
    "#from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#speaker_ids = TextMelLoader(\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/mel_train_taca2.txt\", hparams).speaker_ids\n",
    "#speaker_ids = TextMelLoader(r\"D:\\ClipperDatasetV2/filelists/mel_train_taca2.txt\", hparams, check_files=False, TBPTT=False).speaker_ids\n",
    "#print(str(speaker_ids).replace(\", \",\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load TorchMoji for Style Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Use torchMoji to score texts for emoji distribution.\n",
    "\n",
    "The resulting emoji ids (0-63) correspond to the mapping\n",
    "in emoji_overview.png file at the root of the torchMoji repo.\n",
    "\n",
    "Writes the result to a csv file.\n",
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "maxlen = 180\n",
    "texts = [\"Testing!\",]\n",
    "\n",
    "with torch.no_grad():\n",
    "    st = SentenceTokenizer(vocabulary, maxlen, ignore_sentences_with_only_custom=True)\n",
    "    torchmoji = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "    tokenized, _, _ = st.tokenize_sentences(texts) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "    embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "    print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a measure for Alignment quality in inferred clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    if not max_len:\n",
    "        max_len = torch.max(lengths).long()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device, dtype=torch.int64)\n",
    "    mask = (ids < lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "# New MUCH more performant version, (doesn't support unique padded inputs, just iterate over the batch dim or smthn if you need padded inputs cause this is still way faster)\n",
    "# @torch.jit.script # should work and be even faster, but makes it harder to debug and it's already fast enough right now\n",
    "def alignment_metric(alignments, input_lengths=None, output_lengths=None, average_across_batch=False):\n",
    "    alignments = alignments.transpose(1,2) # [B, dec, enc] -> [B, enc, dec]\n",
    "    # alignments [batch size, x, y]\n",
    "    # input_lengths [batch size] for len_x\n",
    "    # output_lengths [batch size] for len_y\n",
    "    if input_lengths == None:\n",
    "        input_lengths =  torch.ones(alignments.size(0), device=alignments.device)*(alignments.shape[1]-1) # [B] # 147\n",
    "    if output_lengths == None:\n",
    "        output_lengths = torch.ones(alignments.size(0), device=alignments.device)*(alignments.shape[2]-1) # [B] # 767\n",
    "    batch_size = alignments.size(0)\n",
    "    optimums = torch.sqrt(input_lengths.double()**2 + output_lengths.double()**2).view(batch_size)\n",
    "    \n",
    "    # [B, enc, dec] -> [B, dec], [B, dec]\n",
    "    values, cur_idxs = torch.max(alignments, 1) # get max value in column and location of max value\n",
    "    \n",
    "    cur_idxs = cur_idxs.float()\n",
    "    prev_indx = torch.cat((cur_idxs[:,0][:,None], cur_idxs[:,:-1]), dim=1) # shift entire tensor right by one.\n",
    "    dist = ((prev_indx - cur_idxs).pow(2) + 1).pow(0.5)\n",
    "    dist.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=dist.size(1)), 0.0) # remove padding\n",
    "    dist = dist.sum(dim=(1)) # remove padding\n",
    "    diagonalitys = (dist + 1.4142135)/optimums # remove padding\n",
    "    \n",
    "    alignments.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=alignments.size(2))[:,None,:], 0.0)\n",
    "    encoder_max_focus = torch.sum(alignments, dim=2).max(dim=1)[0] # [B, enc, dec] -> [B, sum_enc] -> [B]\n",
    "    encoder_min_focus = torch.sum(alignments, dim=2).min(dim=1)[0]\n",
    "    encoder_avg_focus = torch.sum(alignments, dim=2).mean(dim=1)\n",
    "    \n",
    "    values.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=values.size(1)), 0.0) # because padding\n",
    "    avg_prob = values.mean(dim=1)\n",
    "    avg_prob *= (alignments.size(2)/output_lengths.float()) # because padding\n",
    "    \n",
    "    if average_across_batch:\n",
    "        diagonalitys = diagonalitys.mean()\n",
    "        encoder_max_focus = encoder_max_focus.mean()\n",
    "        encoder_min_focus = encoder_min_focus.mean()\n",
    "        encoder_avg_focus = encoder_avg_focus.mean()\n",
    "        avg_prob = avg_prob.mean()\n",
    "    return diagonalitys.cpu(), avg_prob.cpu(), encoder_max_focus.cpu(), encoder_min_focus.cpu(), encoder_avg_focus.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesize audio (From Filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"They say the eyes are the window to the soul, and you can clearly see the desire burning within them for the treasure you now hold. It's honestly a little frightening. never before have you seen Celestia's perfect composure slip like this, and you find yourself having to push her muzzle away from your face before she starts drooling on you.;\"\n",
    "\"\"\"\n",
    "|Derpy|29\n",
    "|Spitfire|57\n",
    "|Twilight|3\n",
    "|Applejack|4\n",
    "|Rarity|2\n",
    "|RD|1\n",
    "|Pinkie Pie|0\n",
    "|Fluttershy|11\n",
    "|Spike|10\n",
    "|Trixie|13\n",
    "|Celestia|14\n",
    "|Luna|17\n",
    "|Cadance|19\n",
    "|Shining Armor|22\n",
    "\"\"\"\n",
    "\n",
    "clips = \"\"\"\n",
    "|(Game) Them's Fightin' Herds_Oleander|150\n",
    "|(Game) Them's Fightin' Herds_Fred|151\n",
    "|(Game) Them's Fightin' Herds_Pom|152\n",
    "|(Game) Them's Fightin' Herds_Velvet|153\n",
    "|(Game) Them's Fightin' Herds_Arizona|154\n",
    "|(Game) Them's Fightin' Herds_Tianhuo|155\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "|(Show) My Little Pony_Rainbow|160\n",
    "|(Show) My Little Pony_Pinkie|161\n",
    "|(Show) My Little Pony_Rarity|162\n",
    "|(Show) My Little Pony_Spike|163\n",
    "|(Show) My Little Pony_Fluttershy|164\n",
    "|(Show) My Little Pony_Nightmare Moon|165\n",
    "|(Show) Dan Vs_Dan|166\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "|(Show) My Little Pony_Scootaloo|168\n",
    "|(Show) My Little Pony_Big Mac|169\n",
    "|(Show) My Little Pony_Sweetie Belle|170\n",
    "|(Show) My Little Pony_Apple Bloom|171\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "_text_override = \"\"\"Goddamit anon, I knew you couldn't be trusted!\"\"\"\n",
    "_audio_path_override = None\n",
    "_speaker_id_override = None\n",
    "style_mode = 'torchmoji_hidden' # Options = 'mel','token','zeros','torchmoji_hidden','torchmoji_string'\n",
    "\n",
    "acceptable_alignment = 0.76 # threshold to stop retrying\n",
    "acceptable_diagonality = 1.5 # maximum diagonality, 1.04 is average GT. 1.00 is a diagonal line.\n",
    "\n",
    "# Score Parameters\n",
    "diagonality_weighting = 0.5 # 'stutter factor', a penalty for clips where the model jumps back and forwards in the sentence.\n",
    "max_focus_weighting = 1.0   # 'stuck factor', a penalty for clips that spend execisve time on the same letter.\n",
    "min_focus_weighting = 1.0   # 'skip factor', a penalty for skipping/ignoring parts of the input text.\n",
    "max_attempts = 256 # retries at each clip\n",
    "\n",
    "max_decoder_steps = 80\n",
    "max_text_segment_length = 200\n",
    "sigma = 1.0\n",
    "gate_threshold = 0.5\n",
    "gate_delay = 0\n",
    "use_arpabet = 1\n",
    "save_wavs = 1 # saves wavs to infer folder\n",
    "show_audio = 1\n",
    "show_graphs = 1\n",
    "graph_scale = 0.5\n",
    "alignment_graph_width = 3840\n",
    "alignment_graph_height = 1920\n",
    "batch_size = 256\n",
    "\n",
    "model.decoder.gate_delay = gate_delay\n",
    "model.decoder.max_decoder_steps = max_decoder_steps\n",
    "model.decoder.gate_threshold = gate_threshold\n",
    "\n",
    "counter = 0\n",
    "#for clip in clips:\n",
    "while False:\n",
    "    audio_path, text, speaker_id = clip.split(\"|\")\n",
    "    \n",
    "    # one moment DEBUG\n",
    "    #text = _text_override + \" \" + text.split(\", \")[-1] +\" and I will be your helper today.\"\n",
    "    \n",
    "    # use overrides as needed\n",
    "    if _text_override != None:\n",
    "        print(text) # DEBUG\n",
    "        text = _text_override\n",
    "    if _audio_path_override != None:\n",
    "        audio_path = _audio_path_override\n",
    "    if _speaker_id_override != None:\n",
    "        speaker_id = _speaker_id_override\n",
    "    \n",
    "    # get speaker_id (tacotron)\n",
    "    tacotron_speaker_id = tacotron_speaker_id_lookup[int(speaker_id)]\n",
    "    tacotron_speaker_id = torch.LongTensor([tacotron_speaker_id]).cuda().repeat(batch_size)\n",
    "    \n",
    "    # get speaker_id (waveglow)\n",
    "    waveglow_speaker_id = waveglow_speaker_id_lookup[int(speaker_id)]\n",
    "    waveglow_speaker_id = torch.LongTensor([waveglow_speaker_id]).cuda()#.repeat(batch_size)\n",
    "    \n",
    "    # style\n",
    "    if style_mode == 'mel':\n",
    "        mel = load_mel(audio_path.replace(\".npy\",\".wav\")).cuda().half()\n",
    "        style_input = mel\n",
    "    elif style_mode == 'token':\n",
    "        pass\n",
    "        #style_input =\n",
    "    elif style_mode == 'zeros':\n",
    "        style_input = None\n",
    "    elif style_mode == 'torchmoji_hidden':\n",
    "        tokenized, _, _ = st.tokenize_sentences([text,]) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "        embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "        style_input = torch.from_numpy(embedding).cuda().half().repeat(batch_size,1)\n",
    "    elif style_mode == 'torchmoji_string':\n",
    "        style_input = [text,]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # add punc\n",
    "    if text[-1] not in ',.?!;:': text+='.'\n",
    "    \n",
    "    # parse text\n",
    "    text = text.replace(\"...\",\".\")\n",
    "    text = unidecode(text)\n",
    "    print(\"raw_text: \",text)\n",
    "    if use_arpabet: text = ARPA(text)\n",
    "    else: text = f\"☺{text}␤\"\n",
    "    print(\"model_input: \",text)\n",
    "    \n",
    "    print(\"-------------------------------------------------\")\n",
    "    with torch.no_grad():\n",
    "        sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :] # convert text to tensor representation\n",
    "        sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long().repeat(batch_size, 1)\n",
    "        for i in range(1):\n",
    "            # text, speaker_ids, style_mel=None, style_int=None, style_token=None, style_torchmoji_hidden=None, style_torchmoji_string=None\n",
    "            avg_prob = torch.tensor(0.0)\n",
    "            best_score=torch.tensor(-9e9)\n",
    "            tries = 0\n",
    "            try:\n",
    "                while avg_prob.item() < acceptable_alignment or diagonality.item() > acceptable_diagonality:\n",
    "                    # run inference\n",
    "                    \n",
    "                    with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "                        mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = model.inference(sequence, tacotron_speaker_id, style_input=style_input, style_mode=style_mode)\n",
    "                    print( prof.key_averages().table(sort_by=\"cuda_time_total\") )\n",
    "                    print(\"------------------------------------------------\")\n",
    "                    print( prof.table(sort_by=\"cuda_time_total\") )\n",
    "                    prof.export_chrome_trace(r\"D:\\TacotronPPP1.3\\tracing_inplaceoptimized.txt\")\n",
    "                    raise\n",
    "                    # find metrics for each item\n",
    "                    gate_batch_outputs[:,:8] = 0 # ignore gate predictions for the first 0.05s\n",
    "                    output_lengths = gate_batch_outputs.argmax(dim=1)\n",
    "                    diagonality_batch, avg_prob_batch, enc_max_focus_batch, enc_min_focus_batch = alignment_metric(alignments_batch, output_lengths=output_lengths)\n",
    "                    \n",
    "                    batch = zip(\n",
    "                        mel_batch_outputs.split(1,dim=0),\n",
    "                        mel_batch_outputs_postnet.split(1,dim=0),\n",
    "                        gate_batch_outputs.split(1,dim=0),\n",
    "                        alignments_batch.split(1,dim=0),)\n",
    "                    # split batch into items\n",
    "                    \n",
    "                    for j, (mel_outputs, mel_outputs_postnet, gate_outputs, alignments) in enumerate(batch):\n",
    "                        diagonality = diagonality_batch[j]\n",
    "                        avg_prob = avg_prob_batch[j]\n",
    "                        enc_max_focus = enc_max_focus_batch[j]\n",
    "                        enc_min_focus = enc_min_focus_batch[j]\n",
    "                        weighted_score = avg_prob.item()-((max(diagonality.item(),1.11)-1.11) * diagonality_weighting + max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting)-(max(0.9-enc_min_focus.item(),0)*min_focus_weighting)\n",
    "                        score_str = f\"{round(diagonality.item(),3)}  {round(avg_prob.item()*100,2)}%  {round(weighted_score,4)}  {round(max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting,2)} {round(max(0.9-enc_min_focus.item(),0),2)}|\"\n",
    "                        if weighted_score > best_score:\n",
    "                            best_score = weighted_score\n",
    "                            best_score_str = score_str\n",
    "                            best_generation = [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "                        print(score_str, end=\"\")\n",
    "                        tries+=1\n",
    "                        if tries > (max_attempts-1): raise StopIteration\n",
    "            except StopIteration:\n",
    "                pass\n",
    "            mel_outputs, mel_outputs_postnet, gate_outputs, alignments = best_generation# pickup whatever was the best attempt\n",
    "            \n",
    "            # remove any padding\n",
    "            mel_outputs = mel_outputs[:,:,:gate_outputs.argmax()]\n",
    "            mel_outputs_postnet = mel_outputs_postnet[:,:,:gate_outputs.argmax()]\n",
    "            alignments = alignments[:,:gate_outputs.argmax(),:]\n",
    "            \n",
    "            speaker_id = speaker_id[:mel_outputs_postnet.size(0)] # remove excess speaker_ids\n",
    "            print(f\"\\nScore: {best_score}\\nStats: {best_score_str}\")\n",
    "            \n",
    "            plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
    "                   alignments.float().data.cpu().numpy()[0].T))\n",
    "            audio = waveglow.infer(mel_outputs_postnet, speaker_ids=waveglow_speaker_id, sigma=sigma)\n",
    "            audio_denoised = denoiser(audio, strength=0.0001)[:, 0]\n",
    "            ipd.display(ipd.Audio(audio.cpu().numpy(), rate=hparams.sampling_rate))        \n",
    "            if save_wavs:\n",
    "                save_audio_path = f\"infer/audio_{counter}.wav\"\n",
    "                if os.path.exists(save_audio_path):\n",
    "                    os.remove(save_audio_path)\n",
    "                librosa.output.write_wav(save_audio_path, np.swapaxes(audio.float().cpu().numpy(),0,1), hparams.sampling_rate)\n",
    "            counter+=1\n",
    "_text = None; _audio_path = None; _speaker_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesize audio (From Text List e.g Fimfic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    # list(chunks([0,1,2,3,4,5,6,7,8,9],2)) -> [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def parse_txt_into_quotes(fpath):\n",
    "    texts = open(fpath, \"r\", encoding=\"utf-8\").read()\n",
    "    \n",
    "    quo ='\"'; texts = [f'\"{text.replace(quo,\"\").strip()}\"' if i%2 else text.replace(quo,\"\").strip() for i, text in enumerate(unidecode(texts).split('\"'))]\n",
    "    \n",
    "    texts_segmented = []\n",
    "    for text in texts:\n",
    "        text = text.strip()\n",
    "        if not len(text.replace('\"','').strip()): continue\n",
    "        text = text\\\n",
    "            .replace(\"\\n\",\" \")\\\n",
    "            .replace(\"  \",\" \")\\\n",
    "            .replace(\"> --------------------------------------------------------------------------\",\"\")\n",
    "        if len(text) > max_text_segment_length:\n",
    "            for seg in [x.strip() for x in text.split(\".\") if len(x.strip()) if x is not '\"']: \n",
    "                if '\"' in text:\n",
    "                    if seg[0] != '\"': seg='\"'+seg\n",
    "                    if seg[-1] != '\"': seg+='\"'\n",
    "                texts_segmented.append(seg)\n",
    "        else:\n",
    "            texts_segmented.append(text.strip())\n",
    "    return texts_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from time import sleep\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\"\"\"\n",
    "|(Game) Them's Fightin' Herds_Oleander|150\n",
    "|(Game) Them's Fightin' Herds_Fred|151\n",
    "|(Game) Them's Fightin' Herds_Pom|152\n",
    "|(Game) Them's Fightin' Herds_Velvet|153\n",
    "|(Game) Them's Fightin' Herds_Arizona|154\n",
    "|(Game) Them's Fightin' Herds_Tianhuo|155\n",
    "|(Game) Elite Dangerous_Eli|156\n",
    "|(Audiobook) A Little Bit Wicked_Skystar|157\n",
    "|(Audiobook) Dr. Who_Doctor|158\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "|(Show) My Little Pony_Rainbow|160\n",
    "|(Show) My Little Pony_Pinkie|161\n",
    "|(Show) My Little Pony_Rarity|162\n",
    "|(Show) My Little Pony_Spike|163\n",
    "|(Show) My Little Pony_Fluttershy|164\n",
    "|(Show) My Little Pony_Nightmare Moon|165\n",
    "|(Show) Dan Vs_Dan|166\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "|(Show) My Little Pony_Scootaloo|168\n",
    "|(Show) My Little Pony_Big Mac|169\n",
    "|(Show) My Little Pony_Sweetie Belle|170\n",
    "|(Show) My Little Pony_Apple Bloom|171\n",
    "\"\"\"\n",
    "\n",
    "speakers = \"\"\"\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "narrators = \"\"\"\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "_audio_path_override = None\n",
    "_speaker_id_override = None\n",
    "style_mode = 'torchmoji_hidden' # Options = 'mel','token','zeros','torchmoji_hidden','torchmoji_string'\n",
    "\n",
    "acceptable_score = 0.8 # sufficient score to just skip ahead instead of checking/generating more outputs\n",
    "absolutely_required_score = 0.3 # retry forever until this score is reached\n",
    "absolute_maximum_tries = 512 # this is per text text input\n",
    "\n",
    "# Score Parameters\n",
    "diagonality_weighting = 0.5 # 'stutter factor', a penalty for clips where the model jumps back and forwards in the sentence.\n",
    "max_focus_weighting = 1.0   # 'stuck factor', a penalty for clips that spend execisve time on the same letter.\n",
    "min_focus_weighting = 1.0   # 'miniskip factor', a penalty for skipping/ignoring small parts of the input text.\n",
    "avg_focus_weighting = 1.0   # 'skip factor', a penalty for skipping very large parts of the input text\n",
    "max_attempts = 256 # retries at each clip # this is per text input\n",
    "batch_size_per_text = 256 # minibatch_size per unique text input\n",
    "simultaneous_texts = 1 # num unique text inputs per batch\n",
    "\n",
    "max_decoder_steps = 1600\n",
    "max_text_segment_length = 120\n",
    "gate_threshold = 0.8\n",
    "gate_delay = 3\n",
    "use_arpabet = 1\n",
    "\n",
    "sigma = 0.95\n",
    "audio_save_path = r\"D:\\Downloads\\infer\\audio\"\n",
    "output_filename = 'Mort Takes a Holiday'\n",
    "save_wavs = 1 # saves wavs to infer folder\n",
    "\n",
    "show_all_attempt_scores = 0\n",
    "show_audio_overwrite_warnings = 1\n",
    "show_input_text = 1\n",
    "show_best_score = 1\n",
    "show_audio  = 1\n",
    "show_graphs = 1\n",
    "status_updates = 1 # ... Done\n",
    "time_to_gen = 1\n",
    "graph_scale = 0.5\n",
    "alignment_graph_width = 3840\n",
    "alignment_graph_height = 1920\n",
    "\n",
    "model.decoder.gate_delay = gate_delay\n",
    "model.decoder.max_decoder_steps = max_decoder_steps\n",
    "model.decoder.gate_threshold = gate_threshold\n",
    "\n",
    "file_path = r\"D:\\Downloads\\infer\\text\\Mort Takes a Holiday.txt\"\n",
    "\n",
    "texts_segmented = parse_txt_into_quotes(file_path)\n",
    "\n",
    "total_len = len(texts_segmented)\n",
    "\n",
    "# 0 init\n",
    "# 1 append\n",
    "# 2 append, generate, blank\n",
    "# 1 append\n",
    "# 2 append, generate, blank\n",
    "# 1\n",
    "# 2\n",
    "\n",
    "continue_from = 0 # skip\n",
    "counter = 0\n",
    "text_batch_in_progress = []\n",
    "for text_index, text in enumerate(texts_segmented):\n",
    "    if text_index < continue_from: print(f\"Skipping {text_index}.\\t\",end=\"\"); counter+=1; continue\n",
    "    print(f\"{text_index}/{total_len}|{datetime.now()}\")\n",
    "    \n",
    "    # setup the text batches\n",
    "    text_batch_in_progress.append(text)\n",
    "    if (len(text_batch_in_progress) == simultaneous_texts) or (text_index == (len(texts_segmented)-1)): # if text batch ready or final input\n",
    "        text_batch = text_batch_in_progress\n",
    "        text_batch_in_progress = []\n",
    "    else:\n",
    "        continue # if batch not ready, add another text\n",
    "    \n",
    "    # pick the speakers for the texts\n",
    "    speaker_ids = [random.choice(speakers).split(\"|\")[2] if ('\"' in text) else random.choice(narrators).split(\"|\")[2] for text in text_batch] # pick speaker if quotemark in text, else narrator\n",
    "    text_batch  = [text.replace('\"',\"\") for text in text_batch] # remove quotes from text\n",
    "    \n",
    "    if _audio_path_override != None:\n",
    "        audio_path = _audio_path_override\n",
    "    if _speaker_id_override != None:\n",
    "        speaker_id = _speaker_id_override\n",
    "    \n",
    "    # get speaker_ids (tacotron)\n",
    "    tacotron_speaker_ids = [tacotron_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    tacotron_speaker_ids = torch.LongTensor(tacotron_speaker_ids).cuda().repeat_interleave(batch_size_per_text)\n",
    "    \n",
    "    # get speaker_ids (waveglow)\n",
    "    waveglow_speaker_ids = [waveglow_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    waveglow_speaker_ids = torch.LongTensor(waveglow_speaker_ids).cuda()\n",
    "    \n",
    "    # style\n",
    "    if style_mode == 'mel':\n",
    "        mel = load_mel(audio_path.replace(\".npy\",\".wav\")).cuda().half()\n",
    "        style_input = mel\n",
    "    elif style_mode == 'token':\n",
    "        pass\n",
    "        #style_input =\n",
    "    elif style_mode == 'zeros':\n",
    "        style_input = None\n",
    "    elif style_mode == 'torchmoji_hidden':\n",
    "        try:\n",
    "            tokenized, _, _ = st.tokenize_sentences(text_batch) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "        except:\n",
    "            raise Exception(f\"text\\n{text_batch}\\nfailed to tokenize.\")\n",
    "        try:\n",
    "            embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "        except Exception as ex:\n",
    "            print(f'Exception: {ex}')\n",
    "            print(f\"text: {text_batch} failed to process.\")\n",
    "            #raise Exception(f\"text\\n{text}\\nfailed to process.\")\n",
    "        style_input = torch.from_numpy(embedding).cuda().half().repeat_interleave(batch_size_per_text, dim=0)\n",
    "    elif style_mode == 'torchmoji_string':\n",
    "        style_input = text_batch\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # check punctuation\n",
    "    valid_last_char = '-,.?!;:' # valid final characters in texts\n",
    "    text_batch = [text+'.' if (text[-1] not in ',.?!;:') else text for text in text_batch]\n",
    "    \n",
    "    # parse text\n",
    "    text_batch = [unidecode(text.replace(\"...\",\". \").replace(\"  \",\" \").strip()) for text in text_batch] # remove eclipses, double spaces, unicode and spaces before/after the text.\n",
    "    if show_input_text: # debug\n",
    "        print(\"raw_text:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    if use_arpabet: # convert texts to ARPAbet (phonetic) versions.\n",
    "        text_batch = [ARPA(text) for text in text_batch]\n",
    "    if show_input_text: # debug\n",
    "        print(\"model_input:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if time_to_gen:\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # convert texts to sequence, pad where appropriate and move to GPU\n",
    "        sequence_split = [torch.LongTensor(text_to_sequence(text, ['english_cleaners'])) for text in text_batch] # convert texts to numpy representation\n",
    "        text_lengths = torch.tensor([seq.size(0) for seq in sequence_split])\n",
    "        max_len = text_lengths.max().item()\n",
    "        sequence = torch.zeros(text_lengths.size(0), max_len).long() # create large tensor to move each text input into\n",
    "        for i in range(text_lengths.size(0)): # move each text into padded input tensor\n",
    "            sequence[i, :sequence_split[i].size(0)] = sequence_split[i]\n",
    "        sequence = sequence.cuda().long().repeat_interleave(batch_size_per_text, dim=0) # move to GPU and repeat text\n",
    "        text_lengths = text_lengths.cuda().long() # move to GPU\n",
    "        print(\"max_len =\", max_len)\n",
    "        print( get_mask_from_lengths(text_lengths).shape )\n",
    "        #sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long().repeat_interleave(batch_size_per_text, 0)# convert numpy to tensor and repeat for each text\n",
    "        \n",
    "        # debug\n",
    "        text_lengths = text_lengths.clone()\n",
    "        sequence = sequence.clone()\n",
    "        \n",
    "        for i in range(1):\n",
    "            try:\n",
    "                best_score = np.ones(simultaneous_texts) * -9e9\n",
    "                tries      = np.zeros(simultaneous_texts)\n",
    "                best_generations = [0]*simultaneous_texts\n",
    "                best_score_str = ['']*simultaneous_texts\n",
    "                while np.amin(best_score) < acceptable_score:\n",
    "                    # run inference\n",
    "                    if status_updates: print(\"Running Tacotron2... \", end='')\n",
    "                    mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = model.inference(sequence, tacotron_speaker_ids, style_input=style_input, style_mode=style_mode, text_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0))\n",
    "                    \n",
    "                    # find metrics for each item\n",
    "                    gate_batch_outputs[:,:20] = 0 # ignore gate predictions for the first 0.05s\n",
    "                    output_lengths = gate_batch_outputs.argmax(dim=1)+gate_delay\n",
    "                    diagonality_batch, avg_prob_batch, enc_max_focus_batch, enc_min_focus_batch, enc_avg_focus_batch = alignment_metric(alignments_batch, input_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0), output_lengths=output_lengths)\n",
    "                    \n",
    "                    batch = list(zip(\n",
    "                        mel_batch_outputs.split(1,dim=0),\n",
    "                        mel_batch_outputs_postnet.split(1,dim=0),\n",
    "                        gate_batch_outputs.split(1,dim=0),\n",
    "                        alignments_batch.split(1,dim=0),\n",
    "                        diagonality_batch,\n",
    "                        avg_prob_batch,\n",
    "                        enc_max_focus_batch,\n",
    "                        enc_min_focus_batch,\n",
    "                        enc_avg_focus_batch,))\n",
    "                    # split batch into items\n",
    "                    \n",
    "                    for j in range(simultaneous_texts): # process each set of text spectrograms seperately\n",
    "                        start, end = (j*batch_size_per_text), ((j+1)*batch_size_per_text)\n",
    "                        sametext_batch = batch[start:end] # seperate the full batch into pieces that use the same input text\n",
    "                        \n",
    "                        # process all items related to the j'th text input\n",
    "                        for k, (mel_outputs, mel_outputs_postnet, gate_outputs, alignments, diagonality, avg_prob, enc_max_focus, enc_min_focus, enc_avg_focus) in enumerate(sametext_batch):\n",
    "                            # factors that make up score\n",
    "                            weighted_score =  avg_prob.item() # general alignment quality\n",
    "                            weighted_score -= (max(diagonality.item(),1.11)-1.11) * diagonality_weighting  # consistent pace\n",
    "                            weighted_score -= max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting # getting stuck on pauses/phones\n",
    "                            weighted_score -= max(0.9-enc_min_focus.item(),0) * min_focus_weighting # skipping single enc outputs\n",
    "                            weighted_score -= max(2.5-enc_avg_focus.item(), 0) * avg_focus_weighting # skipping most enc outputs\n",
    "                            score_str = f\"{round(diagonality.item(),3)} {round(avg_prob.item()*100,2)}% {round(weighted_score,4)} {round(max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting,2)} {round(max(0.9-enc_min_focus.item(),0),2)}|\"\n",
    "                            if weighted_score > best_score[j]:\n",
    "                                best_score[j] = weighted_score\n",
    "                                best_score_str[j] = score_str\n",
    "                                best_generations[j] = [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "                            if show_all_attempt_scores:\n",
    "                                print(score_str, end=\"\")\n",
    "                            tries[j]+=1\n",
    "                            if np.amin(tries) >= (max_attempts-1) and np.amin(best_score) > (absolutely_required_score-1):\n",
    "                                raise StopIteration\n",
    "                            if np.amin(tries) >= absolute_maximum_tries:\n",
    "                                print(f\"Absolutely required score not achieved in {absolute_maximum_tries} attempts - \", end='')\n",
    "                                raise StopIteration\n",
    "                    \n",
    "                    if np.amin(tries) < (max_attempts-1):\n",
    "                        print('Acceptable alignment/diagonality not reached. Retrying.')\n",
    "                    elif np.amin(best_score) < absolutely_required_score:\n",
    "                        print('Score less than absolutely required score. Retrying extra.')\n",
    "            except StopIteration:\n",
    "                del batch\n",
    "                if status_updates: print(\"Done\")\n",
    "                pass\n",
    "            # [[mel, melpost, gate, align], [mel, melpost, gate, align], [mel, melpost, gate, align]] -> [[mel, mel, mel], [melpost, melpost, melpost], [gate, gate, gate], [align, align, align]]\n",
    "            \n",
    "            # zip is being weird so alternative used\n",
    "            mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = [x[0] for x in best_generations], [x[1] for x in best_generations], [x[2] for x in best_generations], [x[3] for x in best_generations] # pickup whatever was the best attempts\n",
    "            \n",
    "            # stack arrays into tensors\n",
    "            gate_batch_outputs = torch.cat(gate_batch_outputs, dim=0)\n",
    "            max_length = torch.max(gate_batch_outputs.argmax(dim=1)) # get highest duration\n",
    "            mel_batch_outputs = torch.cat(mel_batch_outputs, dim=0)[:,:,:max_length]\n",
    "            mel_batch_outputs_postnet = torch.cat(mel_batch_outputs_postnet, dim=0)[:,:,:max_length]\n",
    "            alignments_batch = torch.cat(alignments_batch, dim=0)[:,:max_length,:]\n",
    "            ##print(\"max_length =\", max_length)\n",
    "            ##print(\"gate_batch_outputs.argmax(dim=1) =\", gate_batch_outputs.argmax(dim=1))\n",
    "            ##print(\"mel_batch_outputs.shape\", mel_batch_outputs.shape)\n",
    "            ##print(\"mel_batch_outputs_postnet.shape\", mel_batch_outputs_postnet.shape)\n",
    "            ##print(\"alignments_batch.shape\", alignments_batch.shape)\n",
    "            \n",
    "            if status_updates: print(\"Running WaveGlow... \", end='')\n",
    "            audio_batch = waveglow.infer(mel_batch_outputs_postnet, speaker_ids=waveglow_speaker_ids, sigma=sigma)\n",
    "            audio_denoised_batch = denoiser(audio_batch, strength=0.0001)[:, 0]\n",
    "            if status_updates: print('Done')\n",
    "            \n",
    "            audio_len = 0\n",
    "            for j, (audio, audio_denoised) in enumerate(zip(audio_batch.split(1, dim=0), audio_denoised_batch.split(1, dim=0))):\n",
    "                # remove WaveGlow padding\n",
    "                audio_end = (gate_batch_outputs[j].argmax()+gate_delay) * hparams.hop_length\n",
    "                audio = audio[:,:audio_end]\n",
    "                audio_denoised = audio_denoised[:,:audio_end]\n",
    "                \n",
    "                # remove Tacotron2 padding\n",
    "                spec_end = gate_batch_outputs[j].argmax()+gate_delay\n",
    "                mel_outputs = mel_batch_outputs.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"mel_outputs.split(blah)[j].shape\", mel_outputs.shape)\n",
    "                mel_outputs_postnet = mel_batch_outputs_postnet.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"alignments_batch.shape\", alignments_batch.shape)\n",
    "                alignments = alignments_batch.split(1, dim=0)[j][:,:spec_end,:text_lengths[j]]\n",
    "                ##print(\"alignments.split(blah)[j].shape\", alignments.shape)\n",
    "                \n",
    "                if show_best_score:\n",
    "                    print(f\"Score: {round(best_score[j],3)}\\t\\tStats: {best_score_str[j]}  Verified: {[x.item() for x in alignment_metric(alignments)]}\")\n",
    "                if show_graphs:\n",
    "                    plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
    "                           alignments.float().data.cpu().numpy()[0].T))\n",
    "                if show_audio:\n",
    "                        ipd.display(ipd.Audio(audio.cpu().numpy(), rate=hparams.sampling_rate))        \n",
    "                if save_wavs:\n",
    "                    save_audio_path = os.path.join(audio_save_path,f\"audio_{counter//300:02}_{counter:05}.wav\")\n",
    "                    if os.path.exists(save_audio_path):\n",
    "                        if show_audio_overwrite_warnings:\n",
    "                            print(f\"File already found at [{save_audio_path}], overwriting.\")\n",
    "                        os.remove(save_audio_path)\n",
    "                    if status_updates: print(f\"Saving clip to [{save_audio_path}]... \", end=\"\")\n",
    "                    librosa.output.write_wav(save_audio_path, np.swapaxes(audio.float().cpu().numpy(),0,1), hparams.sampling_rate)\n",
    "                    if status_updates: print(\"Done\")\n",
    "                counter+=1\n",
    "                audio_len+=audio_end\n",
    "            \n",
    "            if time_to_gen:\n",
    "                audio_seconds_generated = round(audio_len.item()/hparams.sampling_rate,3)\n",
    "                print(f\"Took {round(time.time()-start_time,3)}s to generate {audio_seconds_generated}s of audio. (best of {tries} tries)\")\n",
    "                print(\"spec_end/max_len = \", spec_end/max_len)\n",
    "                print(\"spec_end/max_len = \", spec_end/max_len)\n",
    "            \n",
    "            print(\"\")\n",
    "_text = None; _audio_path = None; _speaker_id = None\n",
    "\n",
    "# Merge clips and output the concatenated result\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# get number of intermediate concatenations required (Sox and only merge 340~ files at a time)\n",
    "n_audio_batches = round(len( glob(os.path.join(audio_save_path, \"audio_*_*.wav\")) ) / 300)\n",
    "\n",
    "# ensure path ends in .wav\n",
    "if not output_filename[-4:].lower() == '.wav':\n",
    "    output_filename+='.wav'\n",
    "\n",
    "for i in range(n_audio_batches):\n",
    "    print(f\"Merging audio files {i*300} to {((i+1)*300)-1}... \", end='')\n",
    "    os.system(f'sox {os.path.join(audio_save_path, f\"audio_{i:02}_*.wav\")} -b 16 {os.path.join(audio_save_path, f\"concat_{i:02}.wav\")}')\n",
    "    print(\"Done\")\n",
    "print(f\"Saving output to '{os.path.join(audio_save_path, output_filename)}'... \", end='')\n",
    "os.system(f'sox \"{os.path.join(audio_save_path, \"concat_*.wav\")}\" -b 16 \"{os.path.join(audio_save_path, output_filename)}\"') # merge the merged files into a final output. bit depth of 16 required to go over 4 hour length\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "x = [0,1,2,3]\n",
    "y = ['A','B','C','D']\n",
    "z = list(zip(x,y))\n",
    "\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(type(y))\n",
    "print(y)\n",
    "print(y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "x = [0,1,2,3]\n",
    "y = ['A','B','C','D']\n",
    "z = list(zip(x,y))\n",
    "\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(type(y))\n",
    "print(y)\n",
    "print(y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in zip(torch.rand(5), torch.zeros(5)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([0,1],[1,2]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0]*3\n",
    "x[0] = \"blah\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str text file\n",
    "# arr split by quotes\n",
    "# arr split by periods and quotes based on length\n",
    "# generator[arr] split by text batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show saved Postnet Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "#filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2\"\n",
    "files = sorted(glob(filepath+\"/**/*.mel.npy\", recursive=True))\n",
    "start = int(random.random() * len(files))\n",
    "file_count = 10\n",
    "for i in range(start, start+file_count):\n",
    "    #file = random.choice(files)\n",
    "    file = files[i]\n",
    "    H = np.load(file)\n",
    "    H = (H+5.2)*0.5\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(file+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "    # again\n",
    "    file = file.replace(\".mel.npy\",\".npy\")\n",
    "    H = np.load(file)\n",
    "    H = (H+5.2)*0.5\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(file+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of GT Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "#filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/FiM/S1/s1e1\"\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2\"\n",
    "files = sorted(glob(filepath+\"/**/*.npy\", recursive=True))\n",
    "total_size = 0\n",
    "min_duration = 0.6\n",
    "SR = 48000\n",
    "BD = 2 # 16 bits\n",
    "for path in files:\n",
    "    file_size = os.stat(path).st_size\n",
    "    if file_size > (SR*BD*min_duration):\n",
    "        total_size+=file_size\n",
    "\n",
    "duration = total_size / (SR*BD)\n",
    "duration_min = duration/60\n",
    "duration_hrs = duration/3600\n",
    "total_size_MB = total_size / (1024**3)\n",
    "print(f\"{total_size_MB} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size & Duration of Wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "def get_stats(filepath, ext='.wav'):\n",
    "    import soundfile as sf\n",
    "    files = sorted(glob(filepath+f\"/**/*{ext}\", recursive=True))\n",
    "    total_size = 0\n",
    "    total_duration = 0\n",
    "    min_duration = 0.6\n",
    "    SR = 48000\n",
    "    BD = 2 # 16 bits\n",
    "    for path in files:\n",
    "        file_size = os.stat(path).st_size\n",
    "        audio, samplerate = sf.read(path)\n",
    "        if len(audio)/samplerate > min_duration:\n",
    "            total_size+=file_size\n",
    "            total_duration+=len(audio)/samplerate\n",
    "    duration_min = total_duration/60\n",
    "    duration_hrs = total_duration/3600\n",
    "    print(f\"{total_duration} seconds = {duration_min} minutes = {duration_hrs} hours\")\n",
    "    total_size_MB = total_size / (1024**3)\n",
    "    print(f\"{total_size_MB} GB of wavs\")\n",
    "\n",
    "get_stats(r\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/Blizzard2011\", ext='.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing FP16 vs FP32 individual Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().half()\n",
    "#########################################################\n",
    "def b_inv(b_mat):\n",
    "    eye = b_mat.new_ones(b_mat.size(-1)).diag().expand_as(b_mat).float()\n",
    "    b_inv, _ = torch.solve(eye.float(), b_mat.float())\n",
    "    return b_inv\n",
    "fp16 = b_inv(weight.squeeze())\n",
    "#########################################################\n",
    "print(fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().float()\n",
    "#########################################################\n",
    "fp32 = weight.squeeze().inverse()\n",
    "#########################################################\n",
    "print(fp32)\n",
    "#assert torch.equal(fp32.float(), fp16.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.slogdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().slogdet()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().logdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().det().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10000\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(500,1,1).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().log().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10000\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(500,1,1).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().half().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().half().log()\n",
    "print(log_det_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda()\n",
    "weight.half()\n",
    "weight.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Multiple WaveGlow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from denoiser import Denoiser\n",
    "from glob import glob\n",
    "from random import random\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "from layers import TacotronSTFT, STFT\n",
    "from utils import load_wav_to_torch\n",
    "from hparams import create_hparams\n",
    "%matplotlib inline\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "def plot_audio_spec(audio, sampling_rate=48000):\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def load_model(waveglow_path):\n",
    "    from efficient_model import WaveGlow\n",
    "    from efficient_util import remove_weight_norms\n",
    "    import json\n",
    "    data = r\"\"\"{\n",
    "        \"train_config\": {\n",
    "            \"fp16_run\": false,\n",
    "            \"output_directory\": \"outdir_EfficientBaseline\",\n",
    "            \"epochs\": 1000,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"sigma\": 1.0,\n",
    "            \"iters_per_checkpoint\": 2000,\n",
    "            \"batch_size\": 30,\n",
    "            \"seed\": 1234,\n",
    "            \"checkpoint_path\": \"outdir_EfficientBaseline/waveglow_1425\",\n",
    "            \"with_tensorboard\": true\n",
    "        },\n",
    "        \"data_config\": {\n",
    "            \"training_files\": \"map_0_GT.txt\",\n",
    "            \"segment_length\": 19200,\n",
    "            \"sampling_rate\": 48000,\n",
    "            \"filter_length\": 2400,\n",
    "            \"hop_length\": 600,\n",
    "            \"win_length\": 2400,\n",
    "            \"mel_fmin\": 0.0,\n",
    "            \"mel_fmax\": 16000.0\n",
    "        },\n",
    "        \"dist_config\": {\n",
    "            \"dist_backend\": \"nccl\",\n",
    "            \"dist_url\": \"tcp://127.0.0.1:54321\"\n",
    "        },\n",
    "        \"waveglow_config\": {\n",
    "            \"n_mel_channels\": 160,\n",
    "            \"n_flows\": 12,\n",
    "            \"n_group\": 8,\n",
    "            \"n_early_every\": 4,\n",
    "            \"n_early_size\": 2,\n",
    "            \"memory_efficient\": false,\n",
    "            \"WN_config\": {\n",
    "                \"dilation_channels\":256,\n",
    "                \"residual_channels\":256,\n",
    "                \"skip_channels\":256,\n",
    "                \"n_layers\": 9,\n",
    "                \"radix\": 3,\n",
    "                \"bias\": true\n",
    "            }\n",
    "        }\n",
    "    }\"\"\"\n",
    "    config = json.loads(data)\n",
    "    train_config = config[\"train_config\"]\n",
    "    global data_config\n",
    "    data_config = config[\"data_config\"]\n",
    "    global dist_config\n",
    "    dist_config = config[\"dist_config\"]\n",
    "    global waveglow_config\n",
    "    waveglow_config = { \n",
    "        **config[\"waveglow_config\"], \n",
    "        'win_length': data_config['win_length'],\n",
    "        'hop_length': data_config['hop_length']\n",
    "    }\n",
    "    waveglow = WaveGlow(**waveglow_config).cuda()\n",
    "    waveglow_dict = torch.load(waveglow_path)['model'].state_dict()\n",
    "    waveglow.load_state_dict(waveglow_dict)\n",
    "    waveglow.apply(remove_weight_norms)\n",
    "    waveglow.cuda().eval()#.half()\n",
    "    #for k in waveglow.convinv:\n",
    "    #    k.float()\n",
    "    #denoiser = Denoiser(waveglow)\n",
    "    waveglow_iters = torch.load(waveglow_path)['iteration']\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    return waveglow, waveglow_iters\n",
    "\n",
    "\n",
    "def waveglow_infer(mel_outputs_postnet, sigma_, iters=''):\n",
    "    current_audio = waveglow.infer(mel_outputs_postnet, sigma=sigma_).unsqueeze(0)\n",
    "    audio.append(current_audio)\n",
    "    print(\"sigma = {}\".format(sigma_)); ipd.display(ipd.Audio(audio[len(audio)-1][0].data.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "    maxv = np.iinfo(np.int16).max\n",
    "    sf.write(\"infer/temp.wav\", (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path+f\"\\nAfter WaveGlow {iters}\\nSigma {sigma_}\")\n",
    "\n",
    "\n",
    "def waveglow_infer_filepath(file_path, sigma_, iters=''):\n",
    "    mel_outputs_postnet = np.load(file_path)\n",
    "    mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4\n",
    "    mel_outputs_postnet = torch.from_numpy(mel_outputs_postnet).unsqueeze(0).cuda()#.half()\n",
    "    audio = []\n",
    "    with torch.no_grad():\n",
    "        waveglow_infer(mel_outputs_postnet, sigma_, iters=iters)\n",
    "\n",
    "audio = []\n",
    "hparams = create_hparams()\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*.npy\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"outdir_EfficientBaseline/waveglow_5158\n",
    "outdir_EfficientBaseline/waveglow_5483\n",
    "outdir_EfficientBaseline/waveglow_5666\n",
    "outdir_EfficientBaseline/waveglow_7264\n",
    "outdir_EfficientBias/waveglow_252\n",
    "outdir_EfficientBias/waveglow_655\n",
    "outdir_EfficientBias/waveglow_1003\n",
    "outdir_EfficientBias/waveglow_1935\n",
    "outdir_EfficientBias/waveglow_11346\n",
    "outdir_EfficientBias/waveglow_11863\n",
    "outdir_EfficientBias/waveglow_12118\n",
    "outdir_EfficientBias/waveglow_12282\n",
    "outdir_EfficientBias/waveglow_14197\n",
    "outdir_EfficientBias/waveglow_16058\"\"\"\n",
    "model_paths = r\"\"\"\n",
    "outdir_EfficientBias/waveglow_365\n",
    "\"\"\".split(\"\\n\")\n",
    "\n",
    "disp_mel(load_mel(file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")).squeeze(), desc=file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")+f\"\\nGround Truth\")\n",
    "for model_path in [path for path in model_paths if path]:\n",
    "    waveglow, iters = load_model(f\"/media/cookie/Samsung 860 QVO/TTCheckpoints/waveglow/{model_path}\")\n",
    "    waveglow_infer_filepath(file_path, 0.9, iters=iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveGlow GTA Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from denoiser import Denoiser\n",
    "from glob import glob\n",
    "from random import random\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "from layers import TacotronSTFT, STFT\n",
    "from utils import load_wav_to_torch\n",
    "from hparams import create_hparams\n",
    "from shutil import copyfile\n",
    "\n",
    "from waveglow_utils import PreEmphasis, InversePreEmphasis\n",
    "%matplotlib inline\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\nLength: \"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "def plot_audio_spec(audio, sampling_rate=48000):\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "hparams = create_hparams()\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "#stft_wideband = TacotronSTFT(hparams.filter_length, hparams.hop_length, 1024,\n",
    "#                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "#                    hparams.mel_fmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveglow(waveglow_path):\n",
    "    waveglow_dict = torch.load(waveglow_path)\n",
    "    waveglow = waveglow_dict['model']\n",
    "    waveglow_iters = int(waveglow_dict['iteration'])\n",
    "    waveglow.cuda().eval().half()\n",
    "    denoiser = Denoiser(waveglow)\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    if not hasattr(waveglow, \"spect_scaling\"):\n",
    "        setattr(waveglow, \"spect_scaling\", False)\n",
    "    return waveglow, denoiser, waveglow_iters\n",
    "\n",
    "def load_waveglow_yoyo(waveglow_path):\n",
    "    waveglow_dict = torch.load(waveglow_path)\n",
    "    waveglow = waveglow_dict['model']\n",
    "    waveglow_iters = int(waveglow_dict['iteration'])\n",
    "    waveglow.cuda().eval()#.half()\n",
    "    denoiser = None#Denoiser(waveglow)\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    return waveglow, denoiser, waveglow_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waveglow_dict = torch.load(\"/media/cookie/Samsung 860 QVO/TTCheckpoints/waveglow/outdir_EfficientLarge/best_model\")\n",
    "waveglow = waveglow_dict['model']\n",
    "waveglow_iters = int(waveglow_dict['iteration'])\n",
    "waveglow.cuda().eval()#.half()\n",
    "denoiser = None#Denoiser(waveglow)\n",
    "print(waveglow_iters, \"iterations\")\n",
    "print(waveglow.WNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(waveglow.WNs[0].WN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Random File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveglow_infer(mel_outputs_postnet, sigma_, deempathsis, root_dir='infer', clip_folder='', filename=''):\n",
    "    current_audio = waveglow.infer(mel_outputs_postnet, sigma=sigma_)\n",
    "    if len(current_audio.shape) == 1:\n",
    "        current_audio = current_audio.unsqueeze(0)\n",
    "    if deempathsis:\n",
    "        deempthasis_filter = InversePreEmphasis(float(deempathsis)) # TODO, replace with something lightweight.\n",
    "        current_audio = deempthasis_filter(current_audio.cpu().float().unsqueeze(0)).squeeze(0).cuda()\n",
    "    audio.append(current_audio)\n",
    "    \n",
    "    # Show Audio for Listening in Notebook\n",
    "    #ipd.display(ipd.Audio(audio[len(audio)-1][0].data.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "    \n",
    "    # Save Audio\n",
    "    local_fpath = os.path.join(root_dir, clip_folder, filename) # local filepath\n",
    "    local_dpath = os.path.join(root_dir, clip_folder) # local directory path\n",
    "    os.makedirs(local_dpath, exist_ok=True) # ensure local directory exists\n",
    "    maxv = np.iinfo(np.int16).max # get max int16 value\n",
    "    sf.write(os.path.join(root_dir, \"temp.wav\"), (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate) # write audio to temp\n",
    "    \n",
    "    # Get MSE and MAE\n",
    "    waveglow_spect = load_mel(os.path.join(root_dir, \"temp.wav\")).squeeze() # load spectrogram from wav file.\n",
    "    waveglow_spect_lossy = (waveglow_spect.unsqueeze(0).cuda().half()[:,:,:mel_outputs_postnet.shape[-1]])#+5.2)*0.5 # move spectrogram to GPU, reshape and normalize within -4, 4.\n",
    "    MSE = (nn.MSELoss()(waveglow_spect_lossy, mel_outputs_postnet)).item() # get MSE (Mean Squared Error) between Ground Truth and WaveGlow inferred spectrograms.\n",
    "    MAE = (nn.L1Loss()(waveglow_spect_lossy, mel_outputs_postnet)).item() # get MAE (Mean Absolute Error) between Ground Truth and WaveGlow inferred spectrograms.\n",
    "    \n",
    "    #sf.write(local_fpath+f\"-MSE_{round(MSE,4)}.wav\", (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate) # write audio to fpath\n",
    "    sf.write(local_fpath+f\"-MSE_{round(MSE,4)}.wav\", np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1), hparams.sampling_rate, \"PCM_16\") # write audio to fpath\n",
    "    # Show Spect\n",
    "    #disp_mel(waveglow_spect, desc=f\"\\nAfter WaveGlow\\nSigma: {sigma_}\\nMSELoss: {MSE}\") # Show Plot in Notebook\n",
    "    return MSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*__*.npy\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveglow_infer_paths = [\n",
    "    \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/Special source/s6e24/00_04_52_Rainbow_Neutral__That's a mighty big claim considering everypony here is an amazingly awesome crazy good flyer.mel.npy\"\n",
    "]\n",
    "for i in range(19):\n",
    "    file_path = files[int(random()*len(files))]\n",
    "    waveglow_infer_paths.append(file_path)\n",
    "print(\"\".join([x+\"\\n\" for x in waveglow_infer_paths]))# Print each entry in multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"\"\"\n",
    "# Baseline, 12 Flow, 256 Channel, 8 Layer, 0.00 Empthasis\n",
    "outdir_twilight9/waveglow_140900|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_152256|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_187539|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_229258|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_268276|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_311477|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_334361|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_351862|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_387823|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_422080|1|0|0.00|0\n",
    "# Mini (ReZero), 12 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_30000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_38265|1|0|0.97|0\n",
    "# Mini, 12 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_2598|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_3979|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_6496|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_13894|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_15659|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_35439|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_40000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_55133|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_100000|1|0|0.9|07\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_120000|1|0|0.9|07\n",
    "# Mini, 16 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_11306|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_55528|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_57259|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_69333|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_77184|1|0|0.97|0\n",
    "# Mini, 24 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_66559|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_70000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_80000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_96555|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_108044|1|0|0.97|0\n",
    "# LARGE, 16 Flow, 512 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_EfficientLarge/waveglow_60000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_70000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_80000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_90000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_120119|1|0|0.97|1\n",
    "outdir_EfficientLarge/best_model|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_127217|1|0|0.97|1\n",
    "# Baseline (Nancy Datset Only), 12 Flow, 256 Channel, 8 Layer, 0.00 Empthasis\n",
    "\"\"\"\n",
    "\n",
    "# path|normalize(-4 to 4)|mu_law_quantization|de-empthasis|yoyololicon version\n",
    "waveglow_paths = \"\"\"\n",
    "outdir_twilight9/waveglow_140900|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_152256|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_187539|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_229258|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_268276|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_311477|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_334361|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_351862|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_387823|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_422080|1|0|0.00|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_30000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_38265|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_2598|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_3979|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_6496|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_13894|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_15659|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_35439|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_40000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_55133|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_100000|1|0|0.9|07\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_120000|1|0|0.9|07\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_11306|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_55528|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_57259|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_69333|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_77184|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_66559|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_70000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_80000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_96555|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_108044|1|0|0.97|0\n",
    "outdir_EfficientLarge/waveglow_60000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_70000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_80000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_90000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_120119|1|0|0.97|1\n",
    "outdir_EfficientLarge/best_model|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_127217|1|0|0.97|1\n",
    "outdir_NancyOnly/best_model|0|0|0.00|0\n",
    "\"\"\"[1:-1].split(\"\\n\")\n",
    "\n",
    "print(\"Missing Checkpoints:\")\n",
    "print(\"\\n\".join([x.split(\"|\")[0] for x in waveglow_paths if not os.path.exists(\"../tacotron2/waveglow_latest/\"+x.split(\"|\")[0])]))\n",
    "waveglow_paths = [x for x in waveglow_paths if os.path.exists(\"../tacotron2/waveglow_latest/\"+x.split(\"|\")[0])]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for waveglow_meta in waveglow_paths:\n",
    "        print(\"--------------------------------------------------------\")\n",
    "        waveglow_path, normalize_spec, mu_law_quantization, deempthasis_strength, b_yoyololicon_model = waveglow_meta.split(\"|\")\n",
    "        waveglow_info = str(\"_\".join(waveglow_path.split(\"/\")[0].split(\"_\")[1:]))\n",
    "        print(waveglow_meta)\n",
    "        if b_yoyololicon_model:\n",
    "            waveglow, denoiser, waveglow_current_iter = load_waveglow_yoyo(\"../tacotron2/waveglow_latest/\"+waveglow_path)\n",
    "        else:\n",
    "            waveglow, denoiser, waveglow_current_iter = load_waveglow(\"../tacotron2/waveglow_latest/\"+waveglow_path)\n",
    "        \n",
    "        if not hasattr(waveglow, \"spect_scaling\"):\n",
    "            setattr(waveglow, \"spect_scaling\", False)\n",
    "        \n",
    "        audio = []\n",
    "        total_MAE = total_MSE = 0\n",
    "        best_MAE = best_MSE = 9e9\n",
    "        worst_MAE = worst_MSE = -9e9\n",
    "        for file_path in waveglow_infer_paths:\n",
    "            #print(f\"FILE: {file_path}\") # Print the file path\n",
    "            basename = os.path.splitext(os.path.basename(file_path.replace(\".mel.npy\",\".npy\")))[0] # filename without ext\n",
    "            \n",
    "            mel_outputs_postnet = np.load(file_path) # Load Tacotron2 Postnet Outputs\n",
    "            if int(normalize_spec):\n",
    "                mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4\n",
    "            \n",
    "            #disp_mel(load_mel(file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")).squeeze(), desc=f\"\\nGround Truth\") # Display Ground Truth Spectrogram\n",
    "            #disp_mel(mel_outputs_postnet, desc=\"\\nThis is the original Postnet output from Tacotron\") # Display Tacotron GTA Spectrogram\n",
    "            \n",
    "            mel_outputs_postnet = np.load(file_path.replace(\".mel.npy\",\".npy\")) # Load Ground Truth Spectrogram for inference by WaveGlow.\n",
    "            if int(normalize_spec):\n",
    "                mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4, speeds up initial training\n",
    "            mel_outputs_postnet = torch.from_numpy(mel_outputs_postnet).unsqueeze(0).cuda() # prep tensor for WaveGlow.\n",
    "            if not b_yoyololicon_model:\n",
    "                mel_outputs_postnet = mel_outputs_postnet.half()\n",
    "            \n",
    "            sigma = 0.9\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            sigma = 0.95\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            sigma = 1.0\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            \n",
    "            if not os.path.exists(os.path.join(\"infer\", basename, f'GroundTruth.wav')):\n",
    "                copyfile(os.path.splitext(file_path.replace(\".mel.npy\",\".npy\"))[0]+\".wav\", os.path.join(\"infer\", basename, f'GroundTruth.wav'))\n",
    "            total_MSE+=MSE\n",
    "            best_MSE = min(best_MSE, MSE)\n",
    "            worst_MSE = max(worst_MSE, MSE)\n",
    "            total_MAE+=MAE\n",
    "            best_MAE = min(best_MAE, MAE)\n",
    "            worst_MAE = max(worst_MAE, MAE)\n",
    "        print(f\"Average MSE: {total_MSE/len(waveglow_infer_paths)} Best MSE: {best_MSE} Worst MSE: {worst_MSE}\")\n",
    "        print(f\"Average MAE: {total_MAE/len(waveglow_infer_paths)} Best MAE: {best_MAE} Worst MAE: {worst_MAE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = torch.rand(4096,1024,2,1, device=\"cuda:0\")\n",
    "x = x + 10\n",
    "x = x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = torch.rand(4096,1024,2,1, device=\"cuda:0\")\n",
    "x = x + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(waveglow, \"spect_scaling\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(waveglow.parameters())[0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 2\n",
    "n_group = 8\n",
    "audio = torch.arange(32).repeat(batch_dim, 1)\n",
    "print(audio)\n",
    "audio = audio.view(batch_dim, -1, n_group).transpose(1, 2)\n",
    "print(\"shape =\", audio.shape)\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,6,1)\n",
    "print(x)\n",
    "y_0, y_1 = x.chunk(2,1)\n",
    "print(\"y_0 =\\n\", y_0)\n",
    "print(\"y_1 =\\n\", y_1)\n",
    "\n",
    "n_half = int(x.size(1)/2)\n",
    "y_0 = x[:,:n_half,:]\n",
    "y_1 = x[:,n_half:,:]\n",
    "print(\"y_0 =\\n\", y_0)\n",
    "print(\"y_1 =\\n\", y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)\n",
    "print(x)\n",
    "x.mul_(2).add_(-5)\n",
    "print(x.view(-1))\n",
    "x = x.unsqueeze(0)\n",
    "print(x.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (torch.rand(5)-0.5)*2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x*x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.abs(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\n",
    "    torch.cuda.get_device_properties(\"cuda:0\").total_memory/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_allocated(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.max_memory_allocated(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_reserved(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.max_memory_reserved(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_summary(\"cuda:0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoise_strength = 10\n",
    "for i in audio:\n",
    "    audio_denoised = denoiser(i, strength=denoise_strength)[:, 0]\n",
    "    ipd.display(ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "maxv = np.iinfo(np.int16).max\n",
    "sf.write(\"infer/temp.wav\", (np.swapaxes(audio_denoised.cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate)\n",
    "disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path+f\"\\nAfter Denoise\\nDenoise Strength {denoise_strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,5)\n",
    "print(x)\n",
    "y = x*torch.tensor([0,1,0,1,0])\n",
    "print(y)\n",
    "# masked_fill_ should be a little more performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "truncated_length = 100\n",
    "lengths = torch.tensor([268, 239, 296, 148, 87, 453, 601, 602, 603, 604, 605, 606, 607, 608, 609])\n",
    "processed = 0\n",
    "\n",
    "batch_lengths = lengths[:batch_size]\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"-\"*100)\n",
    "    print(batch_lengths)\n",
    "    print((batch_lengths-truncated_length))\n",
    "    print((batch_lengths-truncated_length)[batch_lengths-truncated_length>0])\n",
    "    print((batch_lengths-truncated_length)[batch_lengths-truncated_length>0].shape[0])\n",
    "    print(batch_size - (batch_lengths-truncated_length)[batch_lengths-truncated_length>0].shape[0])\n",
    "    \n",
    "    #batch_lengths = (batch_lengths-truncated_length)[batch_lengths-truncated_length>0]\n",
    "    print(batch_lengths)\n",
    "    print(\"processed =\",processed)\n",
    "    processed+=batch_size-((batch_lengths-truncated_length)[batch_lengths-truncated_length>0]).shape[0]\n",
    "    batch_lengths = torch.cat((batch_lengths, lengths[processed+batch_size:processed+batch_size+(batch_size-batch_lengths.shape[0])]), 0)\n",
    "    print(batch_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "processed = 0\n",
    "lengths = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "x = lengths[:batch_size]\n",
    "print(x)\n",
    "print(x[x<4].shape[0])\n",
    "x[x<4] = lengths[processed+batch_size:processed+batch_size+x[x<4].shape[0]]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Pre-empthasis for Audio Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from random import random\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from waveglow_utils import PreEmphasis, InversePreEmphasis\n",
    "preempthasis_strength = 0.97\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*__*.wav\", recursive=True))\n",
    "\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/VCTK-Corpus-0.92/wav\"\n",
    "files = sorted(glob(filepath+\"/**/*.wav\", recursive=True))\n",
    "\n",
    "preempth_filter = PreEmphasis(preempthasis_strength).float()\n",
    "deempth_filter = InversePreEmphasis(preempthasis_strength).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio, sample_rate = sf.read(file_path)\n",
    "    print(audio.max())\n",
    "    print(audio.min())\n",
    "    print(\"Original\")\n",
    "    ipd.display(ipd.Audio(audio, rate=sample_rate))\n",
    "    \n",
    "    sf.write(\"infer/temp_original.wav\", audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp_original.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    import scipy\n",
    "    from scipy import signal\n",
    "    sos = signal.butter(10, 60, 'hp', fs=48000, output='sos')\n",
    "    filtered_audio = signal.sosfilt(sos, audio)\n",
    "    \n",
    "    sf.write(\"infer/temp.wav\", filtered_audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    sos = signal.butter(2, 60, 'hp', fs=48000, output='sos')\n",
    "    filtered_audio = signal.sosfilt(sos, audio)\n",
    "    \n",
    "    sf.write(\"infer/temp.wav\", filtered_audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    maxv = np.iinfo(np.int16).max\n",
    "    audio = deempth_filter((torch.tensor(audio)/maxv).unsqueeze(0).unsqueeze(0).float())\n",
    "    print(audio.mean())\n",
    "    print(audio.std())\n",
    "    print(\"De-Empthasis\")\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))\n",
    "    \n",
    "    audio, sample_rate = sf.read(file_path)\n",
    "    audio = preempth_filter((torch.tensor(audio)/maxv).unsqueeze(0).unsqueeze(0).float())\n",
    "    print(\"Pre-Empthasis\")\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))\n",
    "    \n",
    "    print(\"Pre-Empthasis + De-Empthasis\")\n",
    "    audio = deempth_filter(audio)\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mcd(C, C_hat):\n",
    "    \"\"\"C and C_hat are NumPy arrays of shape (T, D),\n",
    "    representing mel-cepstral coefficients.\n",
    "\n",
    "    \"\"\"\n",
    "    K = 10 / np.log(10) * np.sqrt(2)\n",
    "    return K * np.mean(np.sqrt(np.sum((C - C_hat) ** 2, axis=1)))\n",
    "mcd(np.array([[0,0.8,0]]),np.array([[0,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force Loading Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = {\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "    'e':5,\n",
    "}\n",
    "modeldict = {\n",
    "    'c':3,\n",
    "    'e':4,\n",
    "    'f':5,\n",
    "}\n",
    "dummy_modeldict = {k: v for k,v in pretrained.items() if k in modeldict and pretrained[k] == modeldict[k]}\n",
    "model_dict_missing = {k: v for k,v in pretrained.items() if k not in modeldict}\n",
    "model_dict_mismatching = {k: v for k,v in pretrained.items() if k in modeldict and pretrained[k] != modeldict[k]}\n",
    "pretrained_missing = {k: v for k,v in modeldict.items() if k not in pretrained}\n",
    "print(list(model_dict_missing.keys()),'does not exist in the current model')\n",
    "print(list(model_dict_mismatching.keys()),\"is the wrong shape and has been reset\")\n",
    "print(list(pretrained_missing.keys()),\"doesn't have pretrained weights and is reset\")\n",
    "print(dummy_modeldict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Blur to Spectrograms during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_mel(\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/Special source/s6e24/00_03_22_Rainbow_Neutral__tell ya what. I'll leave the teaching stuff to you And I'll just make sure they stay awake.wav\")\n",
    "scale_embed = torch.rand([1,160])*0.5 + 0.55\n",
    "x = x*scale_embed.unsqueeze(2)\n",
    "print(x.shape)\n",
    "print(scale_embed.shape)\n",
    "print(scale_embed)\n",
    "x = x.cuda()\n",
    "disp_mel(x.squeeze().cpu())\n",
    "\n",
    "x_strength = 100.0\n",
    "y_strength = 0.001\n",
    "filter_cycles = 3\n",
    "filter = kornia.filters.GaussianBlur2d((3,3),(x_strength,y_strength))\n",
    "\n",
    "#print(x.shape)\n",
    "# filter input needs (B,C,H,W)\n",
    "out = filter(x.unsqueeze(0))\n",
    "for i in range(filter_cycles-1): out = filter(out)\n",
    "disp_mel(out.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask for Tacotron Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, out=torch.LongTensor(max_len))\n",
    "    mask = (ids < lengths.unsqueeze(1))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,8)\n",
    "output_lengths = torch.tensor([8,5,2,1])\n",
    "print(x)\n",
    "print(output_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ~get_mask_from_lengths(output_lengths)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.expand_as(x)\n",
    "print(y)\n",
    "x.masked_fill_(y, 1e3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
